
@article{darwiche02,
	year = 2002,
	month = {sep},
	publisher = {{AI} Access Foundation},
	volume = {17},
	pages = {229--264},
	author = {A. Darwiche and P. Marquis},
	title = {A Knowledge Compilation Map},
	journal = {Journal of Artificial Intelligence Research}
}

@article{vergari21,
  title={A Compositional Atlas of Tractable Circuit Operations: From Simple Transformations to Complex Information-Theoretic Queries},
  author={Antonio Vergari and YooJung Choi and Anji Liu and Stefano Teso and Guy Van den Broeck},
  year={2021},
  journal   = {CoRR},
  volume    = {abs/2102.06137},
  archivePrefix = {arXiv},
  eprint={2102.06137}
}

@inproceedings{poon11,
  author = {Poon, Hoifung and Domingos, Pedro},
  title = {Sum-Product Networks: A New Deep Architecture},
  year = {2011},
  booktitle = {Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages = {337--346}
}

@article{peharz16,
  title={On the latent variable interpretation in sum-product networks},
  author={Peharz, Robert and Gens, Robert and Pernkopf, Franz and Domingos, Pedro},
  journal={{IEEE} transactions on pattern analysis and machine intelligence},
  volume={39},
  number={10},
  pages={2030--2044},
  year={2016}
}

@inproceedings{conaty17,
  author    = {Diarmaid Conaty and
               Cassio Polpo de Campos and
               Denis Deratani Mau{\'{a}}},
  title     = {Approximation Complexity of Maximum {A} Posteriori Inference in Sum-Product
               Networks},
  booktitle = {Proceedings of the Thirty-Third Conference on Uncertainty in Artificial
               Intelligence},
  year      = {2017},
}

@article{kisa14,
  author = {Doga Kisa and Guy Van den Broeck and Arthur Choi and Adnan Darwiche},
  title = {Probabilistic Sentential Decision Diagrams},
  journal = {Knowledge Representation and Reasoning Conference},
  year = {2014},
  abstract = {We propose the Probabilistic Sentential Decision Diagram (PSDD): A complete and canonical representation of probability distributions defined over the models of a given propositional theory.  Each parameter of a PSDD can be viewed as the (conditional) probability of making a decision in a corresponding Sentential Decision Diagram (SDD). The SDD itself is a recently proposed complete and canonical representation of propositional theories.  We explore a number of interesting properties of PSDDs, including the independencies that underlie them.  We show that the PSDD is a tractable representation.  We further show how the parameters of a PSDD can be efficiently estimated, in closed form, from complete data.  We empirically evaluate the quality of PSDDs learned from data, when we have knowledge, a priori, of the domain logical constraints.},
}

@inproceedings{zhang23,
  title         = {Tractable Control for Autoregressive Language Generation},
  author        = {Zhang, Honghua and Dang, Meihua and Peng, Nanyun and Van den Broeck, Guy},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning (ICML)},
  year          = {2023},
}

@misc{skryagin23,
      title={Scalable Neural-Probabilistic Answer Set Programming},
      author={Arseny Skryagin and Daniel Ochs and Devendra Singh Dhami and Kristian Kersting},
      year={2023},
      eprint={2306.08397},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{kareem22,
 author = {Ahmed, Kareem and Teso, Stefano and Chang, Kai-Wei and Van den Broeck, Guy and Vergari, Antonio},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {29944--29959},
 publisher = {Curran Associates, Inc.},
 title = {Semantic Probabilistic Layers for Neuro-Symbolic Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/c182ec594f38926b7fcb827635b9a8f4-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@article{manhaeve21,
title = {Neural probabilistic logic programming in DeepProbLog},
journal = {Artificial Intelligence},
volume = {298},
pages = {103504},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103504},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221000552},
author = {Robin Manhaeve and Sebastijan Dumančić and Angelika Kimmig and Thomas Demeester and Luc {De Raedt}},
keywords = {Logic, Probability, Neural networks, Probabilistic logic programming, Neuro-symbolic integration, Learning and reasoning},
abstract = {We introduce DeepProbLog, a neural probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques of the underlying probabilistic logic programming language ProbLog can be adapted for the new language. We theoretically and experimentally demonstrate that DeepProbLog supports (i) both symbolic and subsymbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples.}
}

@inproceedings{kamishima03,
  author = {Kamishima, Toshihiro},
  title = {Nantonac Collaborative Filtering: Recommendation Based on Order Responses},
  year = {2003},
  publisher = {Association for Computing Machinery},
  booktitle = {Proceedings of the Ninth {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
}

@article{choi20,
  title = {Probabilistic Circuits: A Unifying Framework for Tractable Probabilistic Models},
  author = {YooJung Choi and Antonio Vergari and Guy Van den Broeck},
  year = {2020},
  journal = {In preparation},
}

@inproceedings{dang20,
  author    = {Meihua Dang and
               Antonio Vergari and
               Guy Van den Broeck},
  title     = {Strudel: Learning Structured-Decomposable Probabilistic Circuits},
  booktitle   = {Proceedings of the 10th International Conference on Probabilistic Graphical Models},
  series = {PGM},
  year      = {2020}
}

@InProceedings{gens13,
  title = 	 {Learning the Structure of Sum-Product Networks},
  author = 	 {Gens, Robert and Domingos, Pedro},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  series = {ICML},
  pages = 	 {873--880},
  year = 	 {2013},
}

@InProceedings{rooshenas14,
  title = 	 {Learning Sum-Product Networks with Direct and Indirect Variable Interactions},
  author = 	 {Rooshenas, Amirmohammad and Lowd, Daniel},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {710--718},
  year = 	 {2014},
  editor = 	 {Xing, Eric P. and Jebara, Tony},
  volume = 	 {32},
  number =       {1},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 jun,
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/rooshenas14.pdf},
  url = 	 {https://proceedings.mlr.press/v32/rooshenas14.html},
  abstract = 	 {Sum-product networks (SPNs) are a deep probabilistic representation that allows for efficient, exact inference.  SPNs generalize many other tractable models, including thin junction trees, latent tree models, and many types of mixtures.  Previous work on learning SPN structure has mainly focused on using top-down or bottom-up clustering to find mixtures, which capture variable interactions indirectly through implicit latent variables.  In contrast, most work on learning graphical models, thin junction trees, and arithmetic circuits has focused on finding direct interactions among variables.  In this paper, we present ID-SPN, a new algorithm for learning SPN structure that unifies the two approaches. In experiments on 20 benchmark datasets, we find that the combination of direct and indirect interactions leads to significantly better accuracy than several state-of-the-art algorithms for learning SPNs and other tractable models.}
}

@inproceedings{jaini18a,
  title={Prometheus: Directly learning acyclic directed graph structures for sum-product networks},
  author={Jaini, Priyank and Ghose, Amur and Poupart, Pascal},
  booktitle={International Conference on Probabilistic Graphical Models},
  series = {PGM},
  pages={181--192},
  year={2018}
}

@inproceedings{liang17,
  author    = {Yitao Liang and
               Jessa Bekker and
               Guy Van den Broeck},
  title     = {Learning the Structure of Probabilistic Sentential Decision Diagrams},
  booktitle = {Proceedings of the Thirty-Third Conference on Uncertainty in Artificial
               Intelligence},
  year      = {2017},
}

@InProceedings{peharz20a,
  title = 	 {Random Sum-Product Networks: A Simple and Effective Approach to Probabilistic Deep Learning},
  author =       {Peharz, Robert and Vergari, Antonio and Stelzner, Karl and Molina, Alejandro and Shao, Xiaoting and Trapp, Martin and Kersting, Kristian and Ghahramani, Zoubin},
  booktitle = 	 {Proceedings of The 35th Uncertainty in Artificial Intelligence Conference},
  pages = 	 {334--344},
  year = 	 {2020},
  editor = 	 {Adams, Ryan P. and Gogate, Vibhav},
  volume = 	 {115},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 jul,
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v115/peharz20a/peharz20a.pdf},
  url = 	 {https://proceedings.mlr.press/v115/peharz20a.html},
  abstract = 	 {Sum-product networks (SPNs) are expressive probabilistic models with a rich set of exact and efficient inference routines. However, in order to guarantee exact inference, they require specific structural constraints, which complicate learning SPNs from data. Thereby, most SPN structure learners proposed so far are tedious to tune, do not scale easily, and are not easily integrated with deep learning frameworks. In this paper, we follow a simple “deep learning” approach, by generating unspecialized random structures, scalable to millions of parameters, and subsequently applying GPU-based optimization. Somewhat surprisingly, our models often perform on par with state-of-the-art SPN structure learners and deep neural networks on a diverse range of generative and discriminative scenarios. At the same time, our models yield well-calibrated uncertainties, and stand out among most deep generative and discriminative models in being robust to missing features and being able to detect anomalies.}
}

@inproceedings{dimauro21,
author = {Nicola Di Mauro and Gennaro Gala and Marco Iannotta and Teresa M. A. Basile},
title = {Random Probabilistic Circuits},
year = {2021},
booktitle = {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence}
}

@InProceedings{geh21a,
  title = 	 {Learning probabilistic sentential decision diagrams under logic constraints by sampling and averaging},
  author =       {Geh, Renato Lui and Mau\'a, Denis Deratani},
  booktitle = 	 {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {2039--2049},
  year = 	 {2021},
  editor = 	 {de Campos, Cassio and Maathuis, Marloes H.},
  volume = 	 {161},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 jul,
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v161/geh21a/geh21a.pdf},
  url = 	 {https://proceedings.mlr.press/v161/geh21a.html},
  abstract = 	 {Probabilistic Sentential Decision Diagrams (PSDDs) are effective tools for combining uncertain knowledge in the form of (learned) probabilities and certain knowledge in the form of logical constraints. Despite some promising recent advances in the topic, very little attention has been given to the problem of effectively learning PSDDs from data and logical constraints in large domains. In this paper, we show that a simple strategy of sampling and averaging PSDDs leads to state-of-the-art performance in many tasks. We overcome some of the issues with previous methods by employing a top-down generation of circuits from a logic formula represented as a BDD. We discuss how to locally grow the circuit while achieving a good trade-off between complexity and goodness-of-fit of the resulting model. Generalization error is further decreased by aggregating sampled circuits through an ensemble of models. Experiments with various domains show that the approach efficiently learns good models even in very low data regimes, while remaining competitive for large sample sizes.}
}

@InProceedings{geh21b,
  title = {Fast And Accurate Learning of Probabilistic Circuits by Random Projections},
  author = {Renato Lui Geh and Denis Deratani Mau{\'a}},
  booktitle = 	 {The 4th Tractable Probabilistic Modeling Workshop},
  year = 	 {2021},
}

@article{mattei20b,
title = {Tractable inference in credal sentential decision diagrams},
journal = {International Journal of Approximate Reasoning},
volume = {125},
pages = {26-48},
year = {2020},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2020.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X20301845},
author = {Lilith Mattei and Alessandro Antonucci and Denis Deratani Mauá and Alessandro Facchini and Julissa {Villanueva Llerena}},
keywords = {Probabilistic graphical models, Imprecise probability, Credal sets, Probabilistic circuits, Sentential decision diagrams, Sum-product networks},
abstract = {Probabilistic sentential decision diagrams are logic circuits where the inputs of disjunctive gates are annotated by probability values. They allow for a compact representation of joint probability mass functions defined over sets of Boolean variables, that are also consistent with the logical constraints defined by the circuit. The probabilities in such a model are usually “learned” from a set of observations. This leads to overconfident and prior-dependent inferences when data are scarce, unreliable or conflicting. In this work, we develop the credal sentential decision diagrams, a generalisation of their probabilistic counterpart that allows for replacing the local probabilities with (so-called credal) sets of mass functions. These models induce a joint credal set over the set of Boolean variables, that sharply assigns probability zero to states inconsistent with the logical constraints. Three inference algorithms are derived for these models. These allow to compute: (i) the lower and upper probabilities of an observation for an arbitrary number of variables; (ii) the lower and upper conditional probabilities for the state of a single variable given an observation; (iii) whether or not all the probabilistic sentential decision diagrams compatible with the credal specification have the same most probable explanation of a given set of variables given an observation of the other variables. These inferences are tractable, as all the three algorithms, based on bottom-up traversal with local linear programming tasks on the disjunctive gates, can be solved in polynomial time with respect to the circuit size. The first algorithm is always exact, while the remaining two might induce a conservative (outer) approximation in the case of multiply connected circuits. A semantics for this approximation together with an auxiliary algorithm able to decide whether or not the result is exact is also provided together with a brute-force characterization of the exact inference in these cases. For a first empirical validation, we consider a simple application based on noisy seven-segment display images. The credal models are observed to properly distinguish between easy and hard-to-detect instances and outperform other generative models not able to cope with logical constraints.}
}

@inproceedings{shen17,
  author    = {Yujia Shen and
               Arthur Choi and
               Adnan Darwiche},
  title     = {A Tractable Probabilistic Model for Subset Selection},
  booktitle = {Proceedings of the Thirty-Third Conference on Uncertainty in Artificial
               Intelligence},
  year      = {2017},
}

@inproceedings{choi15,
  author    = {Arthur Choi and
               Guy Van den Broeck and
               Adnan Darwiche},
  title     = {Tractable Learning for Structured Probability Spaces: {A} Case Study
               in Learning Preference Distributions},
  booktitle = {Proceedings of the Twenty-Fourth International Joint Conference on
               Artificial Intelligence},
  pages     = {2861--2868},
  year      = {2015},
}

@Article{giunchiglia2023,
author={Giunchiglia, Eleonora
and Stoian, Mihaela C{\u{a}}t{\u{a}}lina
and Khan, Salman
and Cuzzolin, Fabio
and Lukasiewicz, Thomas},
title={ROAD-R: the autonomous driving dataset with logical requirements},
journal={Machine Learning},
year={2023},
month={Sep},
day={01},
volume={112},
number={9},
pages={3261-3291},
abstract={Neural networks have proven to be very powerful at computer vision tasks. However, they often exhibit unexpected behaviors, acting against background knowledge about the problem at hand. This calls for models (i) able to learn from requirements expressing such background knowledge, and (ii) guaranteed to be compliant with the requirements themselves. Unfortunately, the development of such models is hampered by the lack of real-world datasets equipped with formally specified requirements. In this paper, we introduce the ROad event Awareness Dataset with logical Requirements (ROAD-R), the first publicly available dataset for autonomous driving with requirements expressed as logical constraints. Given ROAD-R, we show that current state-of-the-art models often violate its logical constraints, and that it is possible to exploit them to create models that (i) have a better performance, and (ii) are guaranteed to be compliant with the requirements themselves.},
issn={1573-0565},
doi={10.1007/s10994-023-06322-z},
url={https://doi.org/10.1007/s10994-023-06322-z}
}

