@book{ bayesian-theory,
 author = {Bernardo, J. M. and Smith, A. F. M.},
 title = {Bayesian theory},
 publisher = {Wiley},
 year = {1994}
}

@inproceedings{ zhao16a,
 author = {Zhao, Han and Poupart, Pascal and Gordon, Geoffrey J},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 series = {NeurIPS},
 title = {A Unified Approach for Learning the Parameters of Sum-Product Networks},
 year = {2016}
}

@inproceedings{ randomspn,
 title = {Random Sum-Product Networks: A Simple and Effective Approach to Probabilistic Deep Learning},
 author = {Robert Peharz and Antonio Vergari and  Karl Stelzner and Alejandro Molina and Xiaoting Shao and Martin Trapp and Kristian Kersting and  Zoubin Ghahramani},
 booktitle = {Proceedings of The 35th Uncertainty in Artificial Intelligence Conference ({UAI})},
 year = {2020}
}

@article{vergari21,
      title={A Compositional Atlas of Tractable Circuit Operations: From Simple Transformations to Complex Information-Theoretic Queries},
      author={Antonio Vergari and YooJung Choi and Anji Liu and Stefano Teso and Guy Van den Broeck},
      year={2021},
  journal   = {CoRR},
  volume    = {abs/2102.06137},
  archivePrefix = {arXiv},
   eprint={2102.06137}
}

@article{ Clarke03,
 author = {Bertrand Clarke},
 title = {Comparing {B}ayes Model Averaging and Stacking When Model Approximation Error Cannot be Ignored},
 journal = {Journal of Machine Learning Research},
 volume = {4},
 number = {},
 year = {2003},
 pages = {683--712}
}

@inproceedings{ domingos-icml,
  author = {Pedro Domingos},
  title = {Bayesian averaging of classifiers and the overfitting problem},
  booktitle = {Proceedings of the Seventeenth International Conference on Machine Learning},
  year = {2000}
 }

 @book{ DeGroot,
	author = {M. H. DeGroot},
	title = {Optimal Statistical Decisions},
	publisher = {McGraw-Hill},
	year = {1970}
}

 @misc{ Minka,
 	author = {T. Minka},
	title = {Bayesian model averaging is not model combination},
	note = {MIT Media Lab Note},
	year = {2000}
}

@article{ bma,
 author = {Jennifer A. Hoeting and David Madigan and Adrian E. Raftery and Chris T. Volinsky},
 title = {Bayesian Model Averaging: A Tutorial},
 journal = {Statistical Science},
 volume = {14},
 number = {4},
 year = {1999},
 pages = {382--401}
}

@inproceedings{correia20,
  author = {Alvaro H. C. Correia and Robert Peharz and Cassio de Campos},
  title = {Joints in Random Forests},
  booktitle = {Advances in Neural Information Processing Systems 33 (NeurIPS)},
  year = {2020}
}

@inproceedings{nishino16,
	author = {Masaaki Nishino and Norihito Yasuda and Shin-ichi Minato and Masaaki Nagata},
	title = {Zero-Suppressed Sentential Decision Diagrams},
	booktitle = {{AAAI} Conference on Artificial Intelligence},
	year = {2016}
}

@inproceedings{ Shen2016,
 author = {Shen, Yujia and Choi, Arthur and Darwiche, Adnan},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Tractable Operations for Arithmetic Circuits of Probabilistic Models},
 year = {2016}
}


@article{arnold80,
  author = {Arnold, D. B. and Sleep, M. R.},
  title = {Uniform Random Generation of Balanced Parenthesis Strings},
  year = {1980},
  volume = {2},
  number = {1},
  journal = {ACM Trans. Program. Lang. Syst.},
  month = {January},
  pages = {122--128},
}

@article{darwiche01a,
  author = {Adnan Darwiche},
  title = {On the Tractable Counting of Theory Models and its Application to Truth Maintenance and Belief Revision},
  journal = {Journal of Applied Non-Classical Logics},
  volume = {11},
  number = {1--2},
  pages = {11--34},
  year  = {2001}
}

@inproceedings{pipa08,
  author = {Pipatsrisawat, Knot and Darwiche, Adnan},
  title = {New Compilation Languages Based on Structured Decomposability},
  year = {2008},
  booktitle = {Proceedings of the Twenty-Third {AAAI} Conference on Artificial Intelligence},
  pages = {517--522},
  series = {AAAI’08}
}

@inproceedings{mattei19,
  title={Exploring the space of probabilistic sentential decision diagrams},
  author={Mattei, Lilith and Soares, D{\'e}cio L. and Antonucci, Alessandro and Mau{\'a}, Denis D. and Facchini, Alessandro},
  booktitle={3rd Workshop of Tractable Probabilistic Modeling},
  year={2019}
}

@inproceedings{pipa10,
  author = {Pipatsrisawat, Knot and Darwiche, Adnan},
  title = {A Lower Bound on the Size of Decomposable Negation Normal Form},
  year = {2010},
  booktitle = {Proceedings of the Twenty-Fourth {AAAI} Conference on Artificial Intelligence},
  pages = {345--350}
}

@inproceedings{darwiche11,
  author = {Darwiche, Adnan},
  title = {{SDD}: A New Canonical Representation of Propositional Knowledge Bases},
  year = {2011},
  booktitle = {Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence},
  pages = {819--826}
}

@article{julissa20,
 author = {Julissa Villanueva Llerena and Denis Deratani Mau{\'a}},
 journal = {International Journal of Approximate Reasoning},
 pages = {158--180},
 title = {Efficient Algorithms for Robustness Analysis of Maximum A Posteriori Inference in Selective Sum-Product Networks},
 volume = {126},
 year = {2020}
}

@inproceedings{trapp16,
  title = "Structure Inference in Sum-Product Networks using Infinite Sum-Product Trees",
  author = "Martin Trapp and Robert Peharz and M. Skowron and Tamas Madl and Franz Pernkopf and R. Trappl",
  year = "2016",
  booktitle = "Neural Information Processing Systems workshop",
}

@article{desana16,
  title={Learning Arbitrary Sum-Product Network Leaves with Expectation-Maximization},
  author={Desana, Mattia and Schn{\"o}rr, Christoph},
  journal={arXiv preprint arXiv:1604.07243},
  year={2016}
}

@article{peharz16,
  title={On the latent variable interpretation in sum-product networks},
  author={Peharz, Robert and Gens, Robert and Pernkopf, Franz and Domingos, Pedro},
  journal={{IEEE} transactions on pattern analysis and machine intelligence},
  volume={39},
  number={10},
  pages={2030--2044},
  year={2016}
}

@inproceedings{butz19,
  title={Deep convolutional sum-product networks},
  author={Butz, Cory J and Oliveira, Jhonatan S and dos Santos, Andr{\'e} E and Teixeira, Andr{\'e} L},
  booktitle={Proceedings of the {AAAI} Conference on Artificial Intelligence},
  volume={33},
  pages={3248--3255},
  year={2019}
}

@inproceedings{chan06,
  author    = {Hei Chan and
               Adnan Darwiche},
  title     = {On the Robustness of Most Probable Explanations},
  booktitle = {Proceedings of the 22nd Conference in Uncertainty in Artificial
               Intelligence},
  year      = {2006},
}

@inproceedings{dennis15,
  author = {Dennis, Aaron and Ventura, Dan},
  title = {Greedy Structure Search for Sum-Product Networks},
  year = {2015},
  booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
  pages = {932--938},
}

@InProceedings{peharz15,
  title = 	 {On Theoretical Properties of Sum-Product Networks},
  author = 	 {Robert Peharz and Sebastian Tschiatschek and Franz Pernkopf and Pedro Domingos},
  booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {744--752},
  year = 	 {2015}
}

@article{darwiche03,
  author = {Darwiche, Adnan},
  title = {A Differential Approach to Inference in Bayesian Networks},
  year = {2003},
  volume = {50},
  number = {3},
  journal = {Journal of the {ACM}},
  pages = {280--305},
}

@incollection{delalleau11,
  title = {Shallow vs. Deep Sum-Product Networks},
  author = {Olivier Delalleau and Bengio, Yoshua},
  booktitle = {Advances in Neural Information Processing Systems 24},
  pages = {666--674},
  year = {2011},
}

@InProceedings{zhao15,
  title = 	 {On the Relationship between Sum-Product Networks and {B}ayesian Networks},
  author = 	 {Han Zhao and Mazen Melibari and Pascal Poupart},
  booktitle =  {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {116--124},
  year = 	 {2015},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
}

@InProceedings{ranganath14,
  title = 	 {Black Box Variational Inference},
  author = 	 {Rajesh Ranganath and Sean Gerrish and David Blei},
  booktitle = {Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {814--822},
  year = 	 {2014},
  volume = 	 {33},
  series = 	 {Proceedings of Machine Learning Research}
}

@book{bishop06,
  author    = "Bishop, Christopher M",
  title     = "{Pattern recognition and machine learning}",
  publisher = "Springer",
  address   = "New York, NY",
  series    = "Information science and statistics",
  year      = "2006",
  url       = "https://cds.cern.ch/record/998831",
  note      = "Softcover published in 2016",
}

@article{hoffman13,
  author  = {Matthew D. Hoffman and David M. Blei and Chong Wang and John Paisley},
  title   = {Stochastic Variational Inference},
  journal = {Journal of Machine Learning Research},
  year    = {2013},
  volume  = {14},
  number  = {4},
  pages   = {1303-1347},
  url     = {http://jmlr.org/papers/v14/hoffman13a.html}
}

@article{kullback51,
  title={On information and sufficiency},
  author={Kullback, Solomon and Leibler, Richard A},
  journal={The annals of mathematical statistics},
  volume={22},
  number={1},
  pages={79--86},
  year={1951},
  publisher={JSTOR}
}

@book{robert13,
  title={Monte Carlo statistical methods},
  author={Robert, Christian and Casella, George},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{wainwright08,
  title={Graphical models, exponential families, and variational inference},
  author={Wainwright, Martin J and Jordan, Michael I and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={1},
  number={1--2},
  pages={1--305},
  year={2008},
  publisher={Now Publishers, Inc.}
}

@article{jordan99,
  title={An introduction to variational methods for graphical models},
  author={Jordan, Michael I and Ghahramani, Zoubin and Jaakkola, Tommi S and Saul, Lawrence K},
  journal={Machine learning},
  volume={37},
  number={2},
  pages={183--233},
  year={1999},
  publisher={Springer}
}

@article{geman84,
  author={S. {Geman} and D. {Geman}},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title={Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images},
  year={1984},
  volume={PAMI-6},
  number={6},
  pages={721-741}
}

@article{metropolis53,
  author = {Metropolis, Nicholas  and Rosenbluth, Arianna W.  and Rosenbluth, Marshall N.  and Teller, Augusta H.  and Teller, Edward },
  title = {Equation of State Calculations by Fast Computing Machines},
  journal = {The Journal of Chemical Physics},
  volume = {21},
  number = {6},
  pages = {1087-1092},
  year = {1953}
  }

@book{gelman13,
  title={Bayesian Data Analysis, Third Edition},
  author={Gelman, A. and Carlin, J.B. and Stern, H.S. and Dunson, D.B. and Vehtari, A. and Rubin, D.B.},
  isbn={9781439840955},
  series={Chapman \& Hall/CRC Texts in Statistical Science},
  url={https://books.google.com.br/books?id=ZXL6AQAAQBAJ},
  year={2013},
  publisher={Taylor \& Francis}
}

@inproceedings{liang17,
  author    = {Yitao Liang and
               Jessa Bekker and
               Guy Van den Broeck},
  title     = {Learning the Structure of Probabilistic Sentential Decision Diagrams},
  booktitle = {Proceedings of the Thirty-Third Conference on Uncertainty in Artificial
               Intelligence},
  year      = {2017},
}

@article{kisa14,
  author = {Doga Kisa and Guy Van den Broeck and Arthur Choi and Adnan Darwiche},
  title = {Probabilistic Sentential Decision Diagrams},
  journal = {Knowledge Representation and Reasoning Conference},
  year = {2014},
  abstract = {We propose the Probabilistic Sentential Decision Diagram (PSDD): A complete and canonical representation of probability distributions defined over the models of a given propositional theory.  Each parameter of a PSDD can be viewed as the (conditional) probability of making a decision in a corresponding Sentential Decision Diagram (SDD). The SDD itself is a recently proposed complete and canonical representation of propositional theories.  We explore a number of interesting properties of PSDDs, including the independencies that underlie them.  We show that the PSDD is a tractable representation.  We further show how the parameters of a PSDD can be efficiently estimated, in closed form, from complete data.  We empirically evaluate the quality of PSDDs learned from data, when we have knowledge, a priori, of the domain logical constraints.},
}

@inproceedings{fierens11,
  author    = {Daan Fierens and
               Guy Van den Broeck and
               Ingo Thon and
               Bernd Gutmann and
               Luc De Raedt},
  title     = {Inference in Probabilistic Logic Programs using Weighted {CNF}s},
  booktitle = {Proceedings of the Twenty-Seventh Conference on Uncertainty
               in Artificial Intelligence},
  pages     = {211--220},
  year      = {2011}
}

@inproceedings{broeck11,
  author = {Van Den Broeck, Guy and Taghipour, Nima and Meert, Wannes and Davis, Jesse and De Raedt, Luc},
  title = {Lifted Probabilistic Inference by First-Order Knowledge Compilation},
  year = {2011},
  booktitle = {Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence},
  pages = {2178--2185},
  series = {IJCAI}
}

@inproceedings{ randomdensitytrees,
author = {Ram, Parikshit and Gray, Alexander G.},
title = {Density Estimation Trees},
year = {2011},
booktitle = {Proceedings of the 17th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
pages = {627--635},
series = {KDD}
}

@article{ forestpgms,
 author = {Liu, Han and Xu, Min and Gu, Haijie and Gupta, Anupam and Lafferty, John and Wasserman, Larry},
 title = {Forest Density Estimation},
 year = {2011},
 volume = {12},
 number = {},
 issn = {1532-4435},
 journal = {Journal of Machine Learning Research},
 pages = {907--951}
}

@inproceedings{ KernelTrees,
title = {Retrofitting Decision Tree Classifiers Using Kernel Density Estimation},
author = {Smyth, Padhraic and Gray, Alexander and Fayyad, Usama},
booktitle = {Proceedings of the Twelfth International Conference on Machine Learning},
series = {ICML},
pages = {506-514},
year = {1995},
}

@inproceedings{ boundedbns,
 author = {Elidan, Gal and Gould, Stephen},
 booktitle = {Advances in Neural Information Processing Systems},
 series = {NeurIPS},
 pages = {},
 title = {Learning Bounded Treewidth Bayesian Networks},
 volume = {21},
 year = {2009}
}

@inproceedings{bach01,
author = {Bach, Francis R. and Jordan, Michael I.},
title = {Thin Junction Trees},
year = {2001},
booktitle = {Proceedings of the 14th International Conference on Neural Information Processing Systems},
pages = {569–576},
series = {NeurIPS}
}

@InProceedings{ xcutnets,
author="Di Mauro, Nicola
and Vergari, Antonio
and Basile, Teresa M. A.
and Esposito, Floriana",
editor="Ceci, Michelangelo
and Hollm{\'e}n, Jaakko
and Todorovski, Ljup{\v{c}}o
and Vens, Celine
and D{\v{z}}eroski, Sa{\v{s}}o",
title="Fast and Accurate Density Estimation with Extremely Randomized Cutset Networks",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2017",
pages="203--219"
}

@article{chavira06,
  title={Compiling relational Bayesian networks for exact inference},
  author={Chavira, Mark and Darwiche, Adnan and Jaeger, Manfred},
  journal={International Journal of Approximate Reasoning},
  volume={42},
  number={1-2},
  pages={4--20},
  year={2006}
}

@article{breiman01,
 author = {Leo Breiman},
 title = {Random Forests},
 journal = {Machine Learning},
 volume = {45},
 pages = {5--32},
 year = {2001}
}

@inproceedings{dasgupta08a,
 author = {Freund, Yoav and Dasgupta, Sanjoy and Kabra, Mayank and Verma, Nakul},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 series = {NeurIPS},
 title = {Learning the structure of manifolds using random projections},
 volume = {20},
 year = {2008}
}

@inproceedings{dasgupta08b,
 author = {Sanjoy Dasgupta and Yoav Freund},
 title = {Random projection trees and low dimensional manifolds},
 booktitle = {Proceedings of the fortieth annual {ACM} symposium on Theory of computing},
 series = {STOC},
 pages = {537--546},
 year = {2008}
}

@inproceedings{dhesi10,
 author = {Dhesi, Aman and Kar, Purushottam},
 booktitle = {Advances in Neural Information Processing Systems},
 series = {NeurIPS},
 pages = {},
 title = {Random Projection Trees Revisited},
 volume = {23},
 year = {2010}
}

@inproceedings{jaini18a,
  title={Prometheus: Directly learning acyclic directed graph structures for sum-product networks},
  author={Jaini, Priyank and Ghose, Amur and Poupart, Pascal},
  booktitle={International Conference on Probabilistic Graphical Models},
  series = {PGM},
  pages={181--192},
  year={2018}
}

@incollection{trapp19,
  title = {Bayesian Learning of Sum-Product Networks},
  author = {Trapp, Martin and Peharz, Robert and Ge, Hong and Pernkopf, Franz and Ghahramani, Zoubin},
  booktitle = {Advances in Neural Information Processing Systems 32},
  pages = {6347--6358},
  year = {2019}
}

@InProceedings{rashwan16,
  title = 	 {Online and Distributed Bayesian Moment Matching for Parameter Learning in Sum-Product Networks},
  author = 	 {Abdullah Rashwan and Han Zhao and Pascal Poupart},
  booktitle = 	 {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1469--1477},
  year = 	 {2016},
  editor = 	 {Arthur Gretton and Christian C. Robert},
  volume = 	 {51},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Cadiz, Spain},
  month = 	 {09--11 May},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v51/rashwan16.pdf},
  url = 	 {http://proceedings.mlr.press/v51/rashwan16.html},
  abstract = 	 {Probabilistic graphical models provide a general and flexible framework for reasoning about complex dependencies in noisy domains with many variables.  Among the various types of probabilistic graphical models, sum-product networks (SPNs) have recently generated some interest because exact inference can always be done in linear time with respect to the size of the network. This is particularly attractive since it means that learning an SPN from data always yields a tractable model for inference. However, existing parameter learning algorithms for SPNs operate in batch mode and do not scale easily to large datasets. In this work, we explore online algorithms to ensure that parameter learning can also be done tractably with respect to the amount of data.  More specifically, we propose a new Bayesian moment matching (BMM) algorithm that operates naturally in an online fashion and that can be easily distributed. We demonstrate the effectiveness and scalability of BMM in comparison to online extensions of gradient descent, exponentiated gradient and expectation maximization on 20 classic benchmarks and 4 large scale datasets.}
}

@InProceedings{zhao16b,
  title = 	 {Collapsed Variational Inference for Sum-Product Networks},
  author = 	 {Han Zhao and Tameem Adel and Geoff Gordon and Brandon Amos},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1310--1318},
  year = 	 {2016},
  editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 jun,
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/zhaoa16.pdf},
  url = 	 {http://proceedings.mlr.press/v48/zhaoa16.html},
  abstract = 	 {Sum-Product Networks (SPNs) are probabilistic inference machines that admit exact inference in linear time in the size of the network. Existing parameter learning approaches for SPNs are largely based on the maximum likelihood principle and are subject to overfitting compared to more Bayesian approaches. Exact Bayesian posterior inference for SPNs is computationally intractable. Even approximation techniques such as standard variational inference and posterior sampling for SPNs are computationally infeasible even for networks of moderate size due to the large number of local latent variables per instance. In this work, we propose a novel deterministic collapsed variational inference algorithm for SPNs that is computationally efficient, easy to implement and at the same time allows us to incorporate prior information into the optimization formulation. Extensive experiments show a significant improvement in accuracy compared with a maximum likelihood based approach.}
}

@InProceedings{butz18,
  author="Butz, Cory J.  and dos Santos, Andr{\'e} E.  and Oliveira, Jhonatan S.  and Stavrinides, John",
  editor="Mouhoub, Malek and Sadaoui, Samira and Ait Mohamed, Otmane and Ali, Moonis",
  title="Efficient Examination of Soil Bacteria Using Probabilistic Graphical Models",
  booktitle="Recent Trends and Future Technology in Applied Intelligence",
  year="2018",
  publisher="Springer International Publishing",
  address="Cham",
  pages="315--326",
  abstract="This paper describes a novel approach to study bacterial relationships in soil datasets using probabilistic graphical models. We demonstrate how to access and reformat publicly available datasets in order to apply machine learning techniques. We first learn a Bayesian network in order to read independencies in linear time between bacterial community characteristics. These independencies are useful in understanding the semantic relationships between bacteria within communities. Next, we learn a Sum-Product network in order to perform inference in linear time. Here, inference can be conducted to answer traditional queries, involving posterior probabilities, or MPE queries, requesting the most likely values of the non-evidence variables given evidence. Our results extend the literature by showing that known relationships between soil bacteria holding in one or a few datasets in fact hold across at least 3500 diverse datasets. This study paves the way for future large-scale studies of agricultural, health, and environmental applications, for which data are publicly available.",
  isbn="978-3-319-92058-0"
}

@article{ratajczak18,
  title={Sum-product networks for sequence labeling},
  author={Ratajczak, Martin and Tschiatschek, Sebastian and Pernkopf, Franz},
  journal={arXiv preprint arXiv:1807.02324},
  year={2018}
}

@inproceedings{zheng18,
  title={Learning graph-structured sum-product networks for probabilistic semantic maps},
  author={Zheng, Kaiyu and Pronobis, Andrzej and Rao, Rajesh PN},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@incollection{gens12,
  title = {Discriminative Learning of Sum-Product Networks},
  author = {Gens, Robert and Domingos, Pedro},
  booktitle = {Advances in Neural Information Processing Systems 25},
  publisher = {NIPS},
  pages = {3239--3247},
  year = {2012},
}

@InProceedings{gens13,
  title = 	 {Learning the Structure of Sum-Product Networks},
  author = 	 {Gens, Robert and Domingos, Pedro},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  series = {ICML},
  pages = 	 {873--880},
  year = 	 {2013},
  }

@incollection{dennis12,
  title = {Learning the Architecture of Sum-Product Networks Using Clustering on Variables},
  author = {Aaron Dennis and Dan Ventura},
  booktitle = {Advances in Neural Information Processing Systems 25},
  publisher = {NIPS},
  pages = {2033--2041},
  year = {2012}
}

@article{peharz18,
  author    = {Robert Peharz and Antonio Vergari and Karl Stelzner and Alejandro Molina and Martin Trapp and Kristian Kersting and Zoubin Ghahramani},
  title     = {Probabilistic Deep Learning using Random Sum-Product Networks},
  journal   = {CoRR},
  volume    = {abs/1806.01910},
  year      = {2018},
  archivePrefix = {arXiv},
  eprint    = {1806.01910}
}

@INPROCEEDINGS{sguerra16,
  author={B. M. {Sguerra} and F. G. {Cozman}},
  booktitle={2016 5th Brazilian Conference on Intelligent Systems (BRACIS)},
  title={Image Classification Using Sum-Product Networks for Autonomous Flight of Micro Aerial Vehicles},
  year={2016},
  pages={139-144}
}

@inproceedings{vergari15,
  title={Simplifying, Regularizing and Strengthening Sum-Product Network Structure Learning},
  author={Antonio Vergari and Nicola Di Mauro and Floriana Esposito},
  booktitle={ECML/PKDD},
  year={2015}
}

@phdthesis{peharz15_phd,
author = {Peharz, Robert},
year = {2015},
title = {Foundations of Sum-Product Networks for Probabilistic
         Modeling},
school = {Graz University of Technology}
}

@inproceedings{conaty17,
  author    = {Diarmaid Conaty and
               Cassio Polpo de Campos and
               Denis Deratani Mau{\'{a}}},
  title     = {Approximation Complexity of Maximum {A} Posteriori Inference in Sum-Product
               Networks},
  booktitle = {Proceedings of the Thirty-Third Conference on Uncertainty in Artificial
               Intelligence},
  year      = {2017},
}

@inproceedings{mei18,
  author = {Jun Mei and Yong Jiang and Kewei Tu},
  title = {Maximum A Posteriori Inference in Sum-Product Networks},
  booktitle = {AAAI Conference on Artificial Intelligence},
  year = {2018},
}

@article{szegedy13,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}

@article{wei18,
  title={Adversarial Examples in Deep Learning: Characterization and Divergence},
  author={Wei, Wenqi and Liu, Ling and Loper, Margaret and Truex, Stacey and Yu, Lei and Emre Gursoy, Mehmet and Wu, Yanzhao},
  journal={arXiv preprint arXiv:1807.00051},
  year={2018}
}

@article{su19,
  title={One pixel attack for fooling deep neural networks},
  author={Su, Jiawei and Vargas, Danilo Vasconcellos and Sakurai, Kouichi},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={23},
  number={5},
  pages={828--841},
  year={2019},
  publisher={IEEE}
}

@inproceedings{guo17,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1321--1330},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{peharz14,
  title={Learning selective sum-product networks},
  author={Peharz, Robert and Gens, Robert and Domingos, Pedro},
  booktitle={Workshop on Learning Tractable Probabilistic Models},
  year={2014}
}

@inproceedings{poon11,
  author = {Poon, Hoifung and Domingos, Pedro},
  title = {Sum-Product Networks: A New Deep Architecture},
  year = {2011},
  booktitle = {Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages = {337--346}
}

@book{pearl1988,
  title = "Probabilistic Reasoning in Intelligent Systems",
  author = "Judea Pearl",
  publisher = "Morgan Kaufmann",
  year = "1988",
}

@InCollection{mccarthy1969,
  author = "John McCarthy and Patrick J. Hayes",
  title = "Some Philosophical Problems from the Standpoint of
  Artificial Intelligence",
  booktitle = "Machine Intelligence 4",
  editor = "B. Meltzer and D. Michie",
  pages = "463--502",
  publisher = "Edinburgh University Press",
  year = "1969",
  note = "reprinted in McC90",
},

@incollection{goodfellow14,
  title = {Generative Adversarial Nets},
  author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle = {Advances in Neural Information Processing Systems 27},
  editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
  pages = {2672--2680},
  year = {2014},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf}
},

@inproceedings{kingma14,
  author    = {Diederik P. Kingma and
               Max Welling},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Auto-Encoding Variational Bayes},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year      = {2014},
  url       = {http://arxiv.org/abs/1312.6114},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
},

@inproceedings{rahman14,
  author = {Rahman, Tahrima and Kothalkar, Prasanna and Gogate, Vibhav},
  title = {Cutset Networks: A Simple, Tractable, and Scalable Approach for Improving the Accuracy of Chow-Liu Trees},
  year = {2014},
  booktitle = {Proceedings of the 2014th European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages = {630--645}
}

@inproceedings{choi15,
  author    = {Arthur Choi and
               Guy Van den Broeck and
               Adnan Darwiche},
  title     = {Tractable Learning for Structured Probability Spaces: {A} Case Study
               in Learning Preference Distributions},
  booktitle = {Proceedings of the Twenty-Fourth International Joint Conference on
               Artificial Intelligence},
  pages     = {2861--2868},
  year      = {2015},
}

@article{shen19,
  author = {Shen, Yujia and Goyanka, Anchal and Darwiche, Adnan and Choi, Arthur},
  year = {2019},
  pages = {7957--7965},
  title = {Structured {B}ayesian Networks: From Inference to Learning with Routes},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
}

@INPROCEEDINGS{chernikova19,
  author={A. {Chernikova} and A. {Oprea} and C. {Nita-Rotaru} and B. {Kim}},
  booktitle={2019 {IEEE} Security and Privacy Workshops},
  title={Are Self-Driving Cars Secure? Evasion Attacks Against Deep Neural Networks for Steering Angle Prediction},
  year={2019},
  volume={},
  number={},
  pages={132--137}
}

@InProceedings{rashwan18a,
  title = 	 {Discriminative Training of Sum-Product Networks by Extended Baum-Welch},
  author = 	 {Abdullah Rashwan and Pascal Poupart and Chen Zhitang},
  booktitle = 	 {Proceedings of the Ninth International Conference on Probabilistic Graphical Models},
  pages = 	 {356--367},
  year = 	 {2018},
  volume = 	 {72},
  series = 	 {Proceedings of Machine Learning Research}
}


@article{bryant86,
  author = {Bryant, Randal E.},
  title = {Graph-Based Algorithms for Boolean Function Manipulation},
  year = {1986},
  volume = {35},
  number = {8},
  journal = {{IEEE} Transactions on Computers},
  pages = {677--691}
}

@ARTICLE{lee59,
  author={C. Y. {Lee}},
  journal={The Bell System Technical Journal},
  title={Representation of switching circuits by binary-decision programs},
  year={1959},
  volume={38},
  number={4},
  pages={985--999}
}

@article{akers78,
  author = {Akers, S. B.},
  title = {Binary Decision Diagrams},
  year = {1978},
  volume = {27},
  number = {6},
  journal = {IEEE Transactions on Computers},
  pages = {509--516}
}

@article{floyd87,
  author = {Bentley, Jon and Floyd, Bob},
  title = {Programming Pearls: A Sample of Brilliance},
  year = {1987},
  volume = {30},
  number = {9},
  journal = {Commun. ACM},
  pages = {754–757},
}

@article{mattei20a,
  title = "Tractable inference in credal sentential decision diagrams",
  journal = "International Journal of Approximate Reasoning",
  volume = "125",
  pages = "26--48",
  year = "2020",
  author = "Lilith Mattei and Alessandro Antonucci and Denis Deratani Mau{\'a} and Alessandro Facchini and Julissa {Villanueva Llerena}"
}

@inproceedings{dang20,
  author    = {Meihua Dang and
               Antonio Vergari and
               Guy Van den Broeck},
  title     = {Strudel: Learning Structured-Decomposable Probabilistic Circuits},
  booktitle   = {Proceedings of the 10th International Conference on Probabilistic Graphical Models},
  series = {PGM},
  year      = {2020}
}

@inproceedings{bekker15,
 author = {Bekker, Jessa and Davis, Jesse and Choi, Arthur and Darwiche, Adnan and Van den Broeck, Guy},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {2242--2250},
 title = {Tractable Learning for Complex Probability Queries},
 year = {2015}
}

@inproceedings{shen17,
  author    = {Yujia Shen and
               Arthur Choi and
               Adnan Darwiche},
  title     = {A Tractable Probabilistic Model for Subset Selection},
  booktitle = {Proceedings of the Thirty-Third Conference on Uncertainty in Artificial
               Intelligence},
  year      = {2017},
}

@inproceedings{choi17,
 author = {Choi, Arthur and Shen, Yujia and Darwiche, Adnan},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {3477--3485},
 title = {Tractability in Structured Probability Spaces},
 volume = {30},
 year = {2017}
}

@inproceedings{choi16,
author = {Choi, Arthur and Tavabi, Nazgol and Darwiche, Adnan},
title = {Structured Features in Naive {B}ayes Classification},
year = {2016},
booktitle = {Proceedings of the Thirtieth {AAAI} Conference on Artificial Intelligence},
pages = {3233--3240},
}

@inproceedings{choi13,
author = {Choi, Arthur and Darwiche, Adnan},
title = {Dynamic Minimization of Sentential Decision Diagrams},
year = {2013},
abstract = {The Sentential Decision Diagram (SDD) is a recently proposed representation of Boolean functions, containing Ordered Binary Decision Diagrams (OBDDs) as a distinguished subclass. While OBDDs are characterized by total variable orders, SDDs are characterized more generally by vtrees. As both OBDDs and SDDs have canonical representations, searching for OBDDs and SDDs of minimal size simplifies to searching for variable orders and vtrees, respectively. For OBDDs, there are effective heuristics for dynamic reordering, based on locally swapping variables. In this paper, we propose an analogous approach for SDDs which navigates the space of vtrees via two operations: one based on tree rotations and a second based on swapping children in a vtree. We propose a particular heuristic for dynamically searching the space of vtrees, showing that it can find SDDs that are an order-of-magnitude more succinct than OBDDs found by dynamic reordering.},
booktitle = {Proceedings of the Twenty-Seventh {AAAI} Conference on Artificial Intelligence},
pages = {187--194}
}

@inproceedings{oztok15,
author = {Oztok, Umut and Darwiche, Adnan},
title = {A Top-down Compiler for Sentential Decision Diagrams},
year = {2015},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
pages = {3141--3148},
}

@article{gatterbauer14,
  author = {Gatterbauer, Wolfgang and Suciu, Dan},
  title = {Oblivious Bounds on the Probability of Boolean Functions},
  year = {2014},
  volume = {39},
  number = {1},
  journal = {ACM Transactions on Database Systems}
}

@inproceedings{karl91,
  author = {Brace, Karl S. and Rudell, Richard L. and Bryant, Randal E.},
  title = {Efficient Implementation of a {BDD} Package},
  year = {1991},
  booktitle = {Proceedings of the 27th {ACM}/{IEEE} Design Automation Conference},
}

@inproceedings{geh20,
 author = {Renato Geh and Denis Mau{\'a} and Alessandro Antonucci},
 title = {Learning Probabilistic Sentential Decision Diagrams by Sampling},
 booktitle = {Proceedings of the VIII Symposium on Knowledge Discovery, Mining and Learning},
 year = {2020},
 publisher = {SBC},
}

@inproceedings{kamishima03,
  author = {Kamishima, Toshihiro},
  title = {Nantonac Collaborative Filtering: Recommendation Based on Order Responses},
  year = {2003},
  publisher = {Association for Computing Machinery},
  booktitle = {Proceedings of the Ninth {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
}

@inproceedings{aloul02,
  author = {Aloul, Fadi A. and Ramani, Arathi and Markov, Igor L. and Sakallah, Karem A.},
  title = {Generic {ILP} versus Specialized 0-1 {ILP}: An Update},
  year = {2002},
  booktitle = {Proceedings of the 2002 {IEEE}/{ACM} International Conference on Computer-Aided Design},
}

@article{een06,
  title={Translating pseudo-boolean constraints into {SAT}},
  author={E{\'e}n, Niklas and S{\"o}rensson, Niklas},
  journal={Journal on Satisfiability, Boolean Modeling and Computation},
  year={2006}
}

@inproceedings{sinz05,
  author = {Sinz, Carsten},
  title = {Towards an Optimal {CNF} Encoding of Boolean Cardinality Constraints},
  year = {2005},
  booktitle = {Proceedings of the 11th International Conference on Principles and Practice of Constraint Programming},
}

@inproceedings{smyth98,
 author = {Smyth, Padhraic and Wolpert, David},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 pages = {},
 publisher = {MIT Press},
 title = {Stacked Density Estimation},
 volume = {10},
 year = {1998}
}

@inproceedings{monteith11,
  author={Monteith, Kristine and Carroll, James L. and Seppi, Kevin and Martinez, Tony},
  booktitle={The 2011 International Joint Conference on Neural Networks},
  title={Turning Bayesian model averaging into Bayesian model combination},
  year={2011},
}

@inproceedings{dang21,
  title   = {Juice: A Julia Package for Logic and Probabilistic Circuits},
  author = {Dang, Meihua and Khosravi, Pasha and Liang, Yitao and Vergari, Antonio and Van den Broeck, Guy},
  booktitle = {Proceedings of the 35th AAAI Conference on Artificial Intelligence (Demo Track)},
  year    = {2021},
  code = "https://github.com/Juice-jl",
}

@INPROCEEDINGS{lowd10,
  author={Lowd, Daniel and Davis, Jesse},
  booktitle={2010 IEEE International Conference on Data Mining},
  title={Learning Markov Network Structure with Decision Trees},
  year={2010},
}

@inproceedings{haaren12,
author = {Van Haaren, Jan and Davis, Jesse},
title = {Markov Network Structure Learning: A Randomized Feature Generation Approach},
year = {2012},
booktitle = {Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence},
}

@inproceedings{dimauro21,
author = {Nicola Di Mauro and Gennaro Gala and Marco Iannotta and Teresa M. A. Basile},
title = {Random Probabilistic Circuits},
year = {2021},
booktitle = {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence}
}

@misc{khosravi20,
  title={Handling Missing Data in Decision Trees: A Probabilistic Approach},
  author={Pasha Khosravi and Antonio Vergari and YooJung Choi and Yitao Liang and Guy Van den Broeck},
  year={2020},
  booktitle={ICML 2020 Artemiss Workshop}
}

@misc{bommasani21,
      title={On the Opportunities and Risks of Foundation Models},
      author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
      year={2021},
      eprint={2108.07258},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gawlikowski21,
      title={A Survey of Uncertainty in Deep Neural Networks},
      author={Jakob Gawlikowski and Cedrique Rovile Njieutcheu Tassi and Mohsin Ali and Jongseok Lee and Matthias Humt and Jianxiang Feng and Anna Kruspe and Rudolph Triebel and Peter Jung and Ribana Roscher and Muhammad Shahzad and Wen Yang and Richard Bamler and Xiao Xiang Zhu},
      year={2021},
      eprint={2107.03342},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{eykholt18,
  title={Robust physical-world attacks on deep learning visual classification},
  author={Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1625--1634},
  year={2018}
}

@inproceedings{ribeiro16,
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939778},
doi = {10.1145/2939672.2939778},
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding
the reasons behind predictions is, however, quite important in assessing trust, which
is fundamental if one plans to take action based on a prediction, or when choosing
whether to deploy a new model. Such understanding also provides insights into the
model, which can be used to transform an untrustworthy model or prediction into a
trustworthy one.In this work, we propose LIME, a novel explanation technique that
explains the predictions of any classifier in an interpretable and faithful manner,
by learning an interpretable model locally varound the prediction. We also propose
a method to explain models by presenting representative individual predictions and
their explanations in a non-redundant way, framing the task as a submodular optimization
problem. We demonstrate the flexibility of these methods by explaining different models
for text (e.g. random forests) and image classification (e.g. neural networks). We
show the utility of explanations via novel experiments, both simulated and with human
subjects, on various scenarios that require trust: deciding if one should trust a
prediction, choosing between models, improving an untrustworthy classifier, and identifying
why a classifier should not be trusted.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135–1144},
numpages = {10},
keywords = {interpretability, black box classifier, explaining machine learning, interpretable machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@misc{yu20,
      title={A Tutorial on VAEs: From Bayes' Rule to Lossless Compression},
      author={Ronald Yu},
      year={2020},
      eprint={2006.10273},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{pctutorial,
  title = {Probabilistic Circuits: Representations, Inference, Learning and Applications},
  author = {Antonio Vergari and YooJung Choi and Robert Peharz and Guy Van den Broeck},
  year = {2020},
  type = {AAAI Tutorial},
  booktitle = {Association for the Advancement of Artificial Intelligence Conference}
}

@article{pclec,
  author    = {Choi, YooJung and Vergari, Antonio and Van den Broeck, Guy},
  title     = {Lecture Notes: Probabilistic Circuits: Representation and Inference},
  month     = Feb,
  year      = {2020},
  url       = "http://starai.cs.ucla.edu/papers/LecNoAAAI20.pdf",
  keywords  = {techreport}
}

@inbook{yaniv19,
author = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D. and Nowozin, Sebastian and Dillon, Joshua V. and Lakshminarayanan, Balaji and Snoek, Jasper},
title = {Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty under Dataset Shift},
year = {2019},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1254},
numpages = {12}
}

@article{papamakarios21,
  author  = {George Papamakarios and Eric Nalisnick and Danilo Jimenez Rezende and Shakir Mohamed and Balaji Lakshminarayanan},
  title   = {Normalizing Flows for Probabilistic Modeling and Inference},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {57},
  pages   = {1-64},
  url     = {http://jmlr.org/papers/v22/19-1028.html}
}

@InProceedings{rezende15,
  title = 	 {Variational Inference with Normalizing Flows},
  author = 	 {Rezende, Danilo and Mohamed, Shakir},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1530--1538},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/rezende15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/rezende15.html},
  abstract = 	 {The choice of the approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.}
}

@inproceedings{gritsenko19,
  author    = {Alexey A. Gritsenko and
               Jasper Snoek and
               Tim Salimans},
  title     = {On the relationship between Normalising Flows and Variational- and
               Denoising Autoencoders},
  booktitle = {Deep Generative Models for Highly Structured Data, {ICLR} 2019 Workshop,
               New Orleans, Louisiana, United States, May 6, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=HklKEUUY\_E},
  timestamp = {Thu, 25 Jul 2019 16:26:32 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/GritsenkoSS19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{rolfe17,
  author    = {Jason Tyler Rolfe},
  title     = {Discrete Variational Autoencoders},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=ryMxXPFex},
  timestamp = {Thu, 25 Jul 2019 14:25:44 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/Rolfe17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{vahdat18b,
 author = {Vahdat, Arash and Andriyash, Evgeny and Macready, William},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {DVAE\#: Discrete Variational Autoencoders with Relaxed Boltzmann Priors},
 url = {https://proceedings.neurips.cc/paper/2018/file/9f53d83ec0691550f7d2507d57f4f5a2-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{vahdat18a,
  author    = {Arash Vahdat and
               William G. Macready and
               Zhengbing Bian and
               Amir Khoshaman and
               Evgeny Andriyash},
  editor    = {Jennifer G. Dy and
               Andreas Krause},
  title     = {{DVAE++:} Discrete Variational Autoencoders with Overlapping Transformations},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
               10-15, 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {5042--5051},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v80/vahdat18a.html},
  timestamp = {Wed, 03 Apr 2019 18:17:30 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/VahdatMBKA18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lippe21,
  author    = {Phillip Lippe and
               Efstratios Gavves},
  title     = {Categorical Normalizing Flows via Continuous Transformations},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=-GLNZeVDuik},
  timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/LippeG21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ziegler19,
  author    = {Zachary M. Ziegler and
               Alexander M. Rush},
  editor    = {Kamalika Chaudhuri and
               Ruslan Salakhutdinov},
  title     = {Latent Normalizing Flows for Discrete Sequences},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {7673--7682},
  publisher = {{PMLR}},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v97/ziegler19a.html},
  timestamp = {Tue, 11 Jun 2019 15:37:38 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/ZieglerR19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@Inbook{dechter98,
author="Dechter, R.",
editor="Jordan, Michael I.",
title="Bucket Elimination: A Unifying Framework for Probabilistic Inference",
bookTitle="Learning in Graphical Models",
year="1998",
publisher="Springer Netherlands",
address="Dordrecht",
pages="75--104",
abstract="Probabilistic inference algorithms for belief updating, finding the most probable explanation, the maximum a posteriori hypothesis, and the maximum expected utility are reformulated within the bucket elimination framework. This emphasizes the principles common to many of the algorithms appearing in the probabilistic inference literature and clarifies the relationship of such algorithms to nonserial dynamic programming algorithms. A general method for combining conditioning and bucket elimination is also presented. For all the algorithms, bounds on complexity are given as a function of the problem's structure.",
isbn="978-94-011-5014-9",
doi="10.1007/978-94-011-5014-9_4",
url="https://doi.org/10.1007/978-94-011-5014-9_4"
}

@book{koller09,
author = {Koller, Daphne and Friedman, Nir},
title = {Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and Machine Learning},
year = {2009},
isbn = {0262013193},
publisher = {The MIT Press},
abstract = {Most tasks require a person or an automated system to reasonto reach conclusions based
on available information. The framework of probabilistic graphical models, presented
in this book, provides a general approach for this task. The approach is model-based,
allowing interpretable models to be constructed and then manipulated by reasoning
algorithms. These models can also be learned automatically from data, allowing the
approach to be used in cases where manually constructing a model is difficult or even
impossible. Because uncertainty is an inescapable aspect of most real-world applications,
the book focuses on probabilistic models, which make the uncertainty explicit and
provide models that are more faithful to reality. Probabilistic Graphical Models discusses
a variety of models, spanning Bayesian networks, undirected Markov networks, discrete
and continuous models, and extensions to deal with dynamical systems and relational
data. For each class of models, the text describes the three fundamental cornerstones:
representation, inference, and learning, presenting both basic concepts and advanced
techniques. Finally, the book considers the use of the proposed framework for causal
reasoning and decision making under uncertainty. The main text in each chapter provides
the detailed technical development of the key ideas. Most chapters also include boxes
with additional material: skill boxes, which describe techniques; case study boxes,
which discuss empirical cases related to the approach described in the text, including
applications in computer vision, robotics, natural language understanding, and computational
biology; and concept boxes, which present significant concepts drawn from the material
in the chapter. Instructors (and readers) can group chapters in various combinations,
from core topics to more technically advanced material, to suit their particular needs.
Adaptive Computation and Machine Learning series}
}

@inproceedings{khosravi19,
  title     = {What to Expect of Classifiers? Reasoning about Logistic Regression with Missing Features},
  author    = {Khosravi, Pasha and Liang, Yitao and Choi, YooJung and Van den Broeck, Guy},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages     = {2716--2724},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/377},
  url       = {https://doi.org/10.24963/ijcai.2019/377},
}

@InProceedings{rooshenas16,
  title = 	 {Discriminative Structure Learning of Arithmetic Circuits},
  author = 	 {Rooshenas, Amirmohammad and Lowd, Daniel},
  booktitle = 	 {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1506--1514},
  year = 	 {2016},
  editor = 	 {Gretton, Arthur and Robert, Christian C.},
  volume = 	 {51},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Cadiz, Spain},
  month = 	 {09--11 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v51/rooshenas16.pdf},
  url = 	 {https://proceedings.mlr.press/v51/rooshenas16.html},
  abstract = 	 {The biggest limitation of probabilistic graphical models is the complexity of inference, which is often intractable.  An appealing alternative is to use tractable probabilistic models, such as arithmetic circuits (ACs) and sum-product networks (SPNs), in which marginal and conditional queries can be answered efficiently.  In this paper, we present the first discriminative structure learning algorithm for ACs, DACLearn (Discriminative AC Learner).  Like previous work on generative structure learning, DACLearn finds a log-linear model with conjunctive features, using the size of an equivalent AC representation as a learning bias.  Unlike previous work, DACLearn optimizes conditional likelihood, resulting in a more accurate conditional distribution.  DACLearn also learns much more compact ACs than generative methods, since it does not need to represent a consistent distribution over the evidence variables.  To ensure efficiency, DACLearn uses novel initialization and search heuristics to drastically reduce the number of feature evaluations required to learn an accurate model.  In experiments on 20 benchmark domains, we find that our DACLearn learns models that are more accurate and compact than other tractable generative and discriminative methods.}
}

@InProceedings{shao20,
  title = 	 {Conditional Sum-Product Networks: Imposing Structure on Deep Probabilistic Architectures},
  author =       {Shao, Xiaoting and Molina, Alejandro and Vergari, Antonio and Stelzner, Karl and Peharz, Robert and Liebig, Thomas and Kersting, Kristian},
  booktitle = 	 {Proceedings of the 10th International Conference on Probabilistic Graphical Models},
  pages = 	 {401--412},
  year = 	 {2020},
  editor = 	 {Jaeger, Manfred and Nielsen, Thomas Dyhre},
  volume = 	 {138},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 sep,
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v138/shao20a/shao20a.pdf},
  url = 	 {https://proceedings.mlr.press/v138/shao20a.html},
  abstract = 	 {Probabilistic graphical models are a central tool in AI, however, they are generally not as expressive
 as deep neural models, and inference is notoriously hard and slow. In contrast, deep probabilistic
 models such as sum-product networks (SPNs) capture joint distributions in a tractable fashion,
 but still lack the expressive power of intractable models based on deep neural networks. Therefore,
 we introduce conditional SPNs (CSPNs), conditional density estimators for multivariate and
 potentially hybrid domains that allow harnessing the expressive power of neural networks while
 still maintaining tractability guarantees. One way to implement CSPNs is to use an existing SPN
 structure and condition its parameters on the input, e.g., via a deep neural network. Our experimental
 evidence demonstrates that CSPNs are competitive with other probabilistic models and yield
 superior performance on multilabel image classification compared to mean field and mixture density
 networks. Furthermore, they can successfully be employed as building blocks for structured
 probabilistic models, such as autoregressive image models.}
}

@article{jaeger04,
author = {Jaeger, Manfred},
title = {Probabilistic Decision Graphs-Combining Verification and AI Techniques for Probabilistic Inference},
year = {2004},
issue_date = {January 2004},
publisher = {World Scientific Publishing Co., Inc.},
address = {USA},
volume = {12},
number = {1 supp},
issn = {0218-4885},
url = {https://doi.org/10.1142/S0218488504002564},
doi = {10.1142/S0218488504002564},
abstract = {We adopt probabilistic decision graphs developed in the field of automated verification
as a tool for probabilistic model representation and inference. We show that probabilistic
inference has linear time complexity in the size of the probabilistic decision graph,
that the smallest probabilistic decision graph for a given distribution is at most
as large as the smallest junction tree for the same distribution, and that in some
cases it can in fact be much smaller. Behind these very promising features of probabilistic
decision graphs lies the fact that they integrate into a single coherent framework
a number of representational and algorithmic optimizations developed for Bayesian
networks (use of hidden variables, context-specific independence, structured representation
of conditional probability tables).},
journal = {Int. J. Uncertain. Fuzziness Knowl.-Based Syst.},
month = jan,
pages = {19–42},
numpages = {24}
}

@article{dechter07,
title = {AND/OR search spaces for graphical models},
journal = {Artificial Intelligence},
volume = {171},
number = {2},
pages = {73-106},
year = {2007},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S000437020600138X},
author = {Rina Dechter and Robert Mateescu},
keywords = {Search, AND/OR search, Decomposition, Graphical models, Bayesian networks, Constraint networks},
abstract = {The paper introduces an AND/OR search space perspective for graphical models that include probabilistic networks (directed or undirected) and constraint networks. In contrast to the traditional (OR) search space view, the AND/OR search tree displays some of the independencies present in the graphical model explicitly and may sometimes reduce the search space exponentially. Indeed, most algorithmic advances in search-based constraint processing and probabilistic inference can be viewed as searching an AND/OR search tree or graph. Familiar parameters such as the depth of a spanning tree, treewidth and pathwidth are shown to play a key role in characterizing the effect of AND/OR search graphs vs. the traditional OR search graphs. We compare memory intensive AND/OR graph search with inference methods, and place various existing algorithms within the AND/OR search space.}
}

@article{choi20,
  title = {Probabilistic Circuits: A Unifying Framework for Tractable Probabilistic Models},
  author = {YooJung Choi and Antonio Vergari and Guy Van den Broeck},
  year = {2020},
  note = {In preparation},
}

@inproceedings{cheng14,
  title={Language modeling with sum-product networks},
  author={Cheng, Wei-Chen and Kok, Stanley and Pham, Hoai Vu and Chieu, Hai Leong and Chai, Kian Ming A.},
  booktitle={Fifteenth Annual Conference of the International Speech Communication Association},
  year={2014}
}

@inproceedings{nath16,
  title={Learning Tractable Probabilistic Models for Fault Localization},
  author={Nath, Aniruddh and Domingos, Pedro M},
  booktitle={Thirtieth AAAI Conference on Artificial Intelligence},
  year={2016}
}

@InProceedings{rooshenas14,
  title = 	 {Learning Sum-Product Networks with Direct and Indirect Variable Interactions},
  author = 	 {Rooshenas, Amirmohammad and Lowd, Daniel},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {710--718},
  year = 	 {2014},
  editor = 	 {Xing, Eric P. and Jebara, Tony},
  volume = 	 {32},
  number =       {1},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 jun,
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/rooshenas14.pdf},
  url = 	 {https://proceedings.mlr.press/v32/rooshenas14.html},
  abstract = 	 {Sum-product networks (SPNs) are a deep probabilistic representation that allows for efficient, exact inference.  SPNs generalize many other tractable models, including thin junction trees, latent tree models, and many types of mixtures.  Previous work on learning SPN structure has mainly focused on using top-down or bottom-up clustering to find mixtures, which capture variable interactions indirectly through implicit latent variables.  In contrast, most work on learning graphical models, thin junction trees, and arithmetic circuits has focused on finding direct interactions among variables.  In this paper, we present ID-SPN, a new algorithm for learning SPN structure that unifies the two approaches. In experiments on 20 benchmark datasets, we find that the combination of direct and indirect interactions leads to significantly better accuracy than several state-of-the-art algorithms for learning SPNs and other tractable models.}
}

@InProceedings{dimauro17a,
author="Di Mauro, Nicola
and Esposito, Floriana
and Ventola, Fabrizio G.
and Vergari, Antonio",
editor="Esposito, Floriana
and Basili, Roberto
and Ferilli, Stefano
and Lisi, Francesca A.",
title="Alternative Variable Splitting Methods to Learn Sum-Product Networks",
booktitle="AI*IA 2017 Advances in Artificial Intelligence",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="334--346",
abstract="Sum-Product Networks (SPNs) are recent deep probabilistic models providing exact and tractable inference. SPNs have been successfully employed as density estimators in several application domains. However, learning an SPN from high dimensional data still poses a challenge in terms of time complexity. This is due to the high cost of determining independencies among random variables (RVs) and sub-populations among samples, two operations that are repeated several times. Even one of the simplest greedy structure learner, LearnSPN, scales quadratically in the number of the variables to determine RVs independencies. In this work we investigate approximate but fast procedures to determine independencies among RVs whose complexity scales in sub-quadratic time. We propose two procedures: a random subspace approach and one that adopts entropy as a criterion to split RVs in linear time. Experimental results prove that LearnSPN equipped by our splitting procedures is able to reduce learning and/or inference times while preserving comparable inference accuracy.",
isbn="978-3-319-70169-1"
}

@InProceedings{peharz20a,
  title = 	 {Random Sum-Product Networks: A Simple and Effective Approach to Probabilistic Deep Learning},
  author =       {Peharz, Robert and Vergari, Antonio and Stelzner, Karl and Molina, Alejandro and Shao, Xiaoting and Trapp, Martin and Kersting, Kristian and Ghahramani, Zoubin},
  booktitle = 	 {Proceedings of The 35th Uncertainty in Artificial Intelligence Conference},
  pages = 	 {334--344},
  year = 	 {2020},
  editor = 	 {Adams, Ryan P. and Gogate, Vibhav},
  volume = 	 {115},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 jul,
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v115/peharz20a/peharz20a.pdf},
  url = 	 {https://proceedings.mlr.press/v115/peharz20a.html},
  abstract = 	 {Sum-product networks (SPNs) are expressive probabilistic models with a rich set of exact and efficient inference routines. However, in order to guarantee exact inference, they require specific structural constraints, which complicate learning SPNs from data. Thereby, most SPN structure learners proposed so far are tedious to tune, do not scale easily, and are not easily integrated with deep learning frameworks. In this paper, we follow a simple “deep learning” approach, by generating unspecialized random structures, scalable to millions of parameters, and subsequently applying GPU-based optimization. Somewhat surprisingly, our models often perform on par with state-of-the-art SPN structure learners and deep neural networks on a diverse range of generative and discriminative scenarios. At the same time, our models yield well-calibrated uncertainties, and stand out among most deep generative and discriminative models in being robust to missing features and being able to detect anomalies.}
}

@InProceedings{geh21a,
  title = 	 {Learning probabilistic sentential decision diagrams under logic constraints by sampling and averaging},
  author =       {Geh, Renato Lui and Mau\'a, Denis Deratani},
  booktitle = 	 {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {2039--2049},
  year = 	 {2021},
  editor = 	 {de Campos, Cassio and Maathuis, Marloes H.},
  volume = 	 {161},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 jul,
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v161/geh21a/geh21a.pdf},
  url = 	 {https://proceedings.mlr.press/v161/geh21a.html},
  abstract = 	 {Probabilistic Sentential Decision Diagrams (PSDDs) are effective tools for combining uncertain knowledge in the form of (learned) probabilities and certain knowledge in the form of logical constraints. Despite some promising recent advances in the topic, very little attention has been given to the problem of effectively learning PSDDs from data and logical constraints in large domains. In this paper, we show that a simple strategy of sampling and averaging PSDDs leads to state-of-the-art performance in many tasks. We overcome some of the issues with previous methods by employing a top-down generation of circuits from a logic formula represented as a BDD. We discuss how to locally grow the circuit while achieving a good trade-off between complexity and goodness-of-fit of the resulting model. Generalization error is further decreased by aggregating sampled circuits through an ensemble of models. Experiments with various domains show that the approach efficiently learns good models even in very low data regimes, while remaining competitive for large sample sizes.}
}

@InProceedings{geh21b,
  title = {Fast And Accurate Learning of Probabilistic Circuits by Random Projections},
  author = {Renato Lui Geh and Denis Deratani Mau{\'a}},
  booktitle = 	 {The 4th Tractable Probabilistic Modeling Workshop},
  year = 	 {2021},
}

@InProceedings{dimauro17b,
author="Di Mauro, Nicola
and Vergari, Antonio
and Basile, Teresa M. A.
and Esposito, Floriana",
editor="Ceci, Michelangelo
and Hollm{\'e}n, Jaakko
and Todorovski, Ljup{\v{c}}o
and Vens, Celine
and D{\v{z}}eroski, Sa{\v{s}}o",
title="Fast and Accurate Density Estimation with Extremely Randomized Cutset Networks",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="203--219",
abstract="Cutset Networks (CNets) are density estimators leveraging context-specific independencies recently introduced to provide exact inference in polynomial time. Learning a CNet is done by firstly building a weighted probabilistic OR tree and then estimating tractable distributions as its leaves. Specifically, selecting an optimal OR split node requires cubic time in the number of the data features, and even approximate heuristics still scale in quadratic time. We introduce Extremely Randomized Cutset Networks (XCNets), CNets whose OR tree is learned by performing random conditioning. This simple yet surprisingly effective approach reduces the complexity of OR node selection to constant time. While the likelihood of an XCNet is slightly worse than an optimally learned CNet, ensembles of XCNets outperform state-of-the-art density estimators on a series of standard benchmark datasets, yet employing only a fraction of the time needed to learn the competitors. Code and data related to this chapter are available at: https://github.com/nicoladimauro/cnet.",
isbn="978-3-319-71249-9"
}

@InProceedings{peharz20b,
  title = 	 {Einsum Networks: Fast and Scalable Learning of Tractable Probabilistic Circuits},
  author =       {Peharz, Robert and Lang, Steven and Vergari, Antonio and Stelzner, Karl and Molina, Alejandro and Trapp, Martin and Van Den Broeck, Guy and Kersting, Kristian and Ghahramani, Zoubin},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {7563--7574},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 jul,
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/peharz20a/peharz20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/peharz20a.html},
  abstract = 	 {Probabilistic circuits (PCs) are a promising avenue for probabilistic modeling, as they permit a wide range of exact and efficient inference routines. Recent “deep-learning-style” implementations of PCs strive for a better scalability, but are still difficult to train on real-world data, due to their sparsely connected computational graphs. In this paper, we propose Einsum Networks (EiNets), a novel implementation design for PCs, improving prior art in several regards. At their core, EiNets combine a large number of arithmetic operations in a single monolithic einsum-operation, leading to speedups and memory savings of up to two orders of magnitude, in comparison to previous implementations. As an algorithmic contribution, we show that the implementation of Expectation-Maximization (EM) can be simplified for PCs, by leveraging automatic differentiation. Furthermore, we demonstrate that EiNets scale well to datasets which were previously out of reach, such as SVHN and CelebA, and that they can be used as faithful generative image models.}
}

@InProceedings{friesen16,
  title = 	 {The Sum-Product Theorem: A Foundation for Learning Tractable Models},
  author = 	 {Friesen, Abram and Domingos, Pedro},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1909--1918},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 jun,
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/friesen16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/friesen16.html},
  abstract = 	 {Inference in expressive probabilistic models is generally intractable, which makes them difficult to learn and limits their applicability. Sum-product networks are a class of deep models where, surprisingly, inference remains tractable even when an arbitrary number of hidden layers are present. In this paper, we generalize this result to a much broader set of learning problems: all those where inference consists of summing a function over a semiring. This includes satisfiability, constraint satisfaction, optimization, integration, and others. In any semiring, for summation to be tractable it suffices that the factors of every product have disjoint scopes. This unifies and extends many previous results in the literature. Enforcing this condition at learning time thus ensures that the learned models are tractable. We illustrate the power and generality of this approach by applying it to a new type of structured prediction problem: learning a nonconvex function that can be globally optimized in polynomial time. We show empirically that this greatly outperforms the standard approach of learning without regard to the cost of optimization.}
}

@book{barwise82,
  title = {Handbook of Mathematical Logic},
  author = {Jon Barwise},
  year = {1982},
}

@inproceedings{friesen15,
  author = {Friesen, Abram L. and Domingos, Pedro},
  title = {Recursive Decomposition for Nonconvex Optimization},
  year = {2015},
  isbn = {9781577357384},
  publisher = {AAAI Press},
  abstract = {Continuous optimization is an important problem in many areas of AI, including vision,
  robotics, probabilistic inference, and machine learning. Unfortunately, most real-world
  optimization problems are nonconvex, causing standard convex techniques to find only
  local optima, even with extensions like random restarts and simulated annealing. We
  observe that, in many cases, the local modes of the objective function have combinatorial
  structure, and thus ideas from combinatorial optimization can be brought to bear.
  Based on this, we propose a problem-decomposition approach to nonconvex optimization.
  Similarly to DPLL-style SAT solvers and recursive conditioning in probabilistic inference,
  our algorithm, RDIS, recursively sets variables so as to simplify and decompose the
  objective function into approximately independent subfunctions, until the remaining
  functions are simple enough to be optimized by standard techniques like gradient descent.
  The variables to set are chosen by graph partitioning, ensuring decomposition whenever
  possible. We show analytically that RDIS can solve a broad class of nonconvex optimization
  problems exponentially faster than gradient descent with random restarts. Experimentally,
  RDIS outperforms standard techniques on problems like structure from motion and protein
  folding.},
  booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
  pages = {253–259},
  numpages = {7},
  location = {Buenos Aires, Argentina},
  series = {IJCAI'15}
}

@article{liang19,
  title={Learning Logistic Circuits},
  volume={33},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/4336},
  DOI={10.1609/aaai.v33i01.33014277},
  abstractNote={This paper proposes a new classification model called logistic circuits. On MNIST and Fashion datasets, our learning algorithm outperforms neural networks that have an order of magnitude more parameters. Yet, logistic circuits have a distinct origin in symbolic AI, forming a discriminative counterpart to probabilistic-logical circuits such as ACs, SPNs, and PSDDs. We show that parameter learning for logistic circuits is convex optimization, and that a simple local search algorithm can induce strong model structures from data.},
  number={01},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Liang, Yitao and Van den Broeck, Guy},
  year={2019},
  month={Jul.},
  pages={4277-4286}
}

@InProceedings{zhang21,
  title = 	 {Probabilistic Generating Circuits},
  author =       {Zhang, Honghua and Juba, Brendan and Van Den Broeck, Guy},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12447--12457},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 jul,
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/zhang21i/zhang21i.pdf},
  url = 	 {https://proceedings.mlr.press/v139/zhang21i.html},
  abstract = 	 {Generating functions, which are widely used in combinatorics and probability theory, encode function values into the coefficients of a polynomial. In this paper, we explore their use as a tractable probabilistic model, and propose probabilistic generating circuits (PGCs) for their efficient representation. PGCs are strictly more expressive efficient than many existing tractable probabilistic models, including determinantal point processes (DPPs), probabilistic circuits (PCs) such as sum-product networks, and tractable graphical models. We contend that PGCs are not just a theoretical framework that unifies vastly different existing models, but also show great potential in modeling realistic data. We exhibit a simple class of PGCs that are not trivially subsumed by simple combinations of PCs and DPPs, and obtain competitive performance on a suite of density estimation benchmarks. We also highlight PGCs’ connection to the theory of strongly Rayleigh distributions.}
}

@InProceedings{maua17a,
  title = 	 {Credal Sum-Product Networks},
  author = 	 {Mauá, Denis D. and Cozman, Fabio G. and Conaty, Diarmaid and Campos, Cassio P.},
  booktitle = 	 {Proceedings of the Tenth International Symposium on Imprecise Probability: Theories and Applications},
  pages = 	 {205--216},
  year = 	 {2017},
  editor = 	 {Antonucci, Alessandro and Corani, Giorgio and Couso, Inés and Destercke, Sébastien},
  volume = 	 {62},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--14 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v62/mauá17a/mauá17a.pdf},
  url = 	 {https://proceedings.mlr.press/v62/mau%C3%A117a.html},
  abstract = 	 {Sum-product networks are a relatively new and increasingly popular class of (precise) probabilistic graphical models that allow for marginal inference with polynomial effort. As with other probabilistic models, sum-product networks are often learned from data and used to perform classification. Hence, their results are prone to be unreliable and overconfident. In this work, we develop credal sum-product networks, an imprecise extension of sum-product networks. We present algorithms and complexity results for common inference tasks. We apply our algorithms on realistic classification task using images of digits and show that credal sum-product networks obtained by a perturbation of the parameters of learned sum-product networks are able to distinguish between reliable and unreliable classifications with high accuracy.}
}

@article{cozman00,
title = {Credal networks},
journal = {Artificial Intelligence},
volume = {120},
number = {2},
pages = {199-233},
year = {2000},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(00)00029-1},
url = {https://www.sciencedirect.com/science/article/pii/S0004370200000291},
author = {Fabio G. Cozman},
keywords = {Graphical models of inference, Convex sets of probability measures, Bayesian networks, Lower and upper expectations, Robust Bayesian analysis, Independence relations, Graphical d-separation relations},
abstract = {This paper presents a complete theory of credal networks, structures that associate convex sets of probability measures with directed acyclic graphs. Credal networks are graphical models for precise/imprecise beliefs. The main contribution of this work is a theory of credal networks that displays as much flexibility and representational power as the theory of standard Bayesian networks. Results in this paper show how to express judgements of irrelevance and independence, and how to compute inferences in credal networks. A credal network admits several extensions—several sets of probability measures comply with the constraints represented by a network. Two types of extensions are investigated. The properties of strong extensions are clarified through a new generalization of d-separation, and exact and approximate inference methods are described for strong extensions. Novel results are presented for natural extensions, and linear fractional programming methods are described for natural extensions. The paper also investigates credal networks that are defined globally through perturbations of a single network.}
}

@InProceedings{sharir18a,
  title = 	 {Sum-Product-Quotient Networks},
  author = 	 {Sharir, Or and Shashua, Amnon},
  booktitle = 	 {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = 	 {529--537},
  year = 	 {2018},
  editor = 	 {Storkey, Amos and Perez-Cruz, Fernando},
  volume = 	 {84},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--11 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v84/sharir18a/sharir18a.pdf},
  url = 	 {https://proceedings.mlr.press/v84/sharir18a.html},
  abstract = 	 {We present a novel tractable generative model that extends Sum-Product Networks (SPNs) and significantly boosts their power. We call it Sum-Product-Quotient Networks (SPQNs), whose  core concept is to incorporate conditional distributions into the model by direct computation using quotient nodes, e.g. $P(A|B) = \frac{P(A,B)}{P(B)}$. We provide sufficient conditions for the tractability of SPQNs that generalize and relax the decomposable and complete tractability conditions of SPNs. These relaxed conditions give rise to an exponential boost to the expressive efficiency of our model, i.e. we prove that there are distributions which SPQNs can compute efficiently but require SPNs to be of exponential size. Thus, we narrow the gap in expressivity between tractable graphical models and other Neural Network-based generative models.}
}

@InProceedings{pevny20a,
  title = 	 {Sum-Product-Transform Networks: Exploiting Symmetries using Invertible Transformations},
  author =       {Pevn\'{y}, Tom\'{a}\v{s} and Sm\'{i}dl, V\'{a}clav and Trapp, Martin and Pol\'{a}\v{c}ek, Ond\v{r}ej and Oberhuber, Tom\'{a}\v{s}},
  booktitle = 	 {Proceedings of the 10th International Conference on Probabilistic Graphical Models},
  pages = 	 {341--352},
  year = 	 {2020},
  editor = 	 {Jaeger, Manfred and Nielsen, Thomas Dyhre},
  volume = 	 {138},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 sep,
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v138/pevny20a/pevny20a.pdf},
  url = 	 {https://proceedings.mlr.press/v138/pevny20a.html},
  abstract = 	 {In this work, we propose Sum-Product-Transform Networks (SPTN), an extension of sum-product networks that uses invertible transformations as additional internal nodes.
 The type and placement of transformations determine properties of the resulting SPTN with many interesting special cases.
 Importantly, SPTN with Gaussian leaves and affine transformations pose the same inference task tractable that can be computed efficiently in SPNs.
 We propose to store and optimize affine transformations in their SVD decompositions using an efficient parametrization of unitary matrices by a set of Givens rotations.
 Last but not least, we demonstrate that G-SPTNs pushes the state-of-the-art on the density estimation task on used datasets.
 }
}

@inproceedings{melibari16a,
author = {Melibari, Mazen and Poupart, Pascal and Doshi, Prashant},
title = {Sum-Product-Max Networks for Tractable Decision Making},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {Investigations into probabilistic graphical models for decision making have predominantly
centered on influence diagrams (IDs) and decision circuits (DCs) for representation
and computation of decision rules that maximize expected utility. Since IDs are typically
handcrafted and DCs are compiled from IDs, in this paper we propose an approach to
learn the structure and parameters of decision-making problems directly from data.
We present a new representation called sum-product-max network (SPMN) that generalizes
a sum-product network (SPN) to the class of decision-making problems and whose solution,
analogous to DCs, scales linearly in the size of the network. We show that SPMNs may
be reduced to DCs linearly and present a first method for learning SPMNs from data.
This approach is significant because it facilitates a novel paradigm of tractable
decision making driven by data.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {1846–1852},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@inproceedings{jaini18b,
 author = {Jaini, Priyank and Poupart, Pascal and Yu, Yaoliang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Homogeneous Mixture Models: Representation, Separation, and Approximation},
 url = {https://proceedings.neurips.cc/paper/2018/file/c5f5c23be1b71adb51ea9dc8e9d444a8-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{martens14,
  author    = {James Martens and
               Venkatesh Medabalimi},
  title     = {On the Expressive Efficiency of Sum Product Networks},
  journal   = {CoRR},
  volume    = {abs/1411.7717},
  year      = {2014},
  url       = {http://arxiv.org/abs/1411.7717},
  eprinttype = {arXiv},
  eprint    = {1411.7717},
  timestamp = {Mon, 13 Aug 2018 16:46:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MartensM14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{darwiche02,
author = {Darwiche, Adnan and Marquis, Pierre},
title = {A Knowledge Compilation Map},
year = {2002},
issue_date = {July 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {17},
number = {1},
issn = {1076-9757},
abstract = {We propose a perspective on knowledge compilation which calls for analyzing different
compilation approaches according to two key dimensions: the succinctness of the target
compilation language, and the class of queries and transformations that the language
supports in polytime. We then provide a knowledge compilation map, which analyzes
a large number of existing target compilation languages according to their succinctness
and their polytime transformations and queries. We argue that such analysis is necessary
for placing new compilation approaches within the context of existing ones. We also
go beyond classical, flat target compilation languages based on CNF and DNF, and consider
a richer, nested class based on directed acyclic graphs (such as OBDDs), which we
show to include a relatively large number of target compilation languages.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {229–264},
numpages = {36}
}

@inproceedings{decampos11,
author = {De Campos, Cassio P.},
title = {New Complexity Results for MAP in Bayesian Networks},
year = {2011},
isbn = {9781577355151},
abstract = {This paper presents new results for the (partial) maximum a posteriori (MAP) problem in Bayesian networks, which is the problem of querying the most probable state configuration of some of the network variables given evidence. It is demonstrated that the problem remains hard even in networks with very simple topology, such as binary polytrees and simple trees (including the Naive Bayes structure), which extends previous complexity results. Furthermore, a Fully Polynomial Time Approximation Scheme for MAP in networks with bounded treewidth and bounded number of states per variable is developed. Approximation schemes were thought to be impossible, but here it is shown otherwise under the assumptions just mentioned, which are adopted in most applications.},
booktitle = {Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence - Volume Volume Three},
pages = {2100–2106},
numpages = {7},
location = {Barcelona, Catalonia, Spain},
series = {IJCAI'11}
}

@inproceedings{darwiche99,
  title={Compiling knowledge into decomposable negation normal form},
  author={Darwiche, Adnan},
  booktitle={IJCAI},
  volume={99},
  pages={284--289},
  year={1999},
}

@article{darwiche01b,
author = {Darwiche, Adnan},
title = {Decomposable Negation Normal Form},
year = {2001},
issue_date = {July 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0004-5411},
url = {https://doi.org/10.1145/502090.502091},
doi = {10.1145/502090.502091},
abstract = {Knowledge compilation has been emerging recently as a new direction of research for dealing with the computational intractability of general propositional reasoning. According to this approach, the reasoning process is split into two phases: an off-line compilation phase and an on-line query-answering phase. In the off-line phase, the propositional theory is compiled into some target language, which is typically a tractable one. In the on-line phase, the compiled target is used to efficiently answer a (potentially) exponential number of queries. The main motivation behind knowledge compilation is to push as much of the computational overhead as possible into the off-line phase, in order to amortize that overhead over all on-line queries. Another motivation behind compilation is to produce very simple on-line reasoning systems, which can be embedded cost-effectively into primitive computational platforms, such as those found in consumer electronics.One of the key aspects of any compilation approach is the target language into which the propositional theory is compiled. Previous target languages included Horn theories, prime implicates/implicants and ordered binary decision diagrams (OBDDs). We propose in this paper a new target compilation language, known as decomposable negation normal form (DNNF), and present a number of its properties that make it of interest to the broad community. Specifically, we show that DNNF is universal; supports a rich set of polynomial--time logical operations; is more space-efficient than OBDDs; and is very simple as far as its structure and algorithms are concerned. Moreover, we present an algorithm for converting any propositional theory in clausal form into a DNNF and show that if the clausal form has a bounded treewidth, then its DNNF compilation has a linear size and can be computed in linear time (treewidth is a graph-theoretic parameter that measures the connectivity of the clausal form). We also propose two techniques for approximating the DNNF compilation of a theory when the size of such compilation is too large to be practical. One of the techniques generates a sound but incomplete compilation, while the other generates a complete but unsound compilation. Together, these approximations bound the exact compilation from below and above in terms of their ability to answer clausal entailment queries. Finally, we show that the class of polynomial--time DNNF operations is rich enough to support relatively complex AI applications, by proposing a specific framework for compiling model-based diagnosis systems.},
journal = {J. ACM},
month = {jul},
pages = {608–647},
numpages = {40},
keywords = {Boolean functions, satisfiability, knowledge compilation, model-based diagnosis, propositional logic}
}

@book{papadimitriou94,
  title={Computational Complexity},
  author={Papadimitriou, C.H.},
  isbn={9780201530827},
  lccn={93005662},
  series={Theoretical computer science},
  year={1994},
  publisher={Addison-Wesley}
}

@inproceedings{gogic95,
author = {Gogic, Goran and Kautz, Henry and Papadimitriou, Christos and Selman, Bart},
title = {The Comparative Linguistics of Knowledge Representation},
year = {1995},
isbn = {1558603638},
abstract = {We develop a methodology for comparing knowledge representation formalisms in terms of their "representational succinctness," that is, their ability to express knowledge situations relatively efficiently. We use this framework for comparing many important formalisms for knowledge base representation: propositional logic, default logic, circumscription, and model preference defaults; and, at a lower level, Horn formulas, characteristic models, decision trees, disjunctive normal form, and conjunctive normal form. We also show that adding new variables improves the effective expressibility of certain knowledge representation formalisms.},
booktitle = {Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 1},
pages = {862–869},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {IJCAI'95}
}

@inproceedings{darwiche20,
  author    = {Adnan Darwiche},
  title     = {Three Modern Roles for Logic in AI},
  year      = {2020},
  booktitle = {Proceedings of the 39th Symposium on Principles of Database Systems (PODS)},
}

@inproceedings{wachter06,
author = {Wachter, Michael and Haenni, Rolf},
title = {Propositional DAGs: A New Graph-Based Language for Representing Boolean Functions},
year = {2006},
isbn = {9781577352716},
abstract = {This paper continues the line of research on knowledge compilation in the context of Negation Normal Forms (NNF) and Binary Decision Diagrams (BDD). The idea is to analyze different target languages according to their succinctness and the classes of queries and transformations supported in polytime. We identify a new property called simple-negation, which is an implicit restriction of all NNFs and BDDs. The removal of this restriction leads to Propositional Directed Acyclic Graphs (PDAG), a more general family of graph-based languages for representing Boolean functions or propositional theories. With respect to certain NNF-based languages, we will show that corresponding PDAG-based languages are at least as succinct and support the same transformations. The most interesting language even supports the same queries and an additional transformation, making it more flexible.},
booktitle = {Proceedings of the Tenth International Conference on Principles of Knowledge Representation and Reasoning},
pages = {277–285},
numpages = {9},
location = {Lake District, UK},
series = {KR'06}
}

@inproceedings{geh19,
 author = {Renato Geh and Denis Mauá},
 title = {End-To-End Imitation Learning of Lane Following Policies Using Sum-Product Networks},
 booktitle = {Anais do XVI Encontro Nacional de Inteligência Artificial e Computacional},
 location = {Salvador},
 year = {2019},
 keywords = {},
 issn = {0000-0000},
 pages = {297--308},
 publisher = {SBC},
 address = {Porto Alegre, RS, Brasil},
 doi = {10.5753/eniac.2019.9292},
 url = {https://sol.sbc.org.br/index.php/eniac/article/view/9292}
}

@INPROCEEDINGS{llerena17,
  author={Llerena, Julissa Villanueva and Deratani Mauá, Denis},
  booktitle={2017 Brazilian Conference on Intelligent Systems (BRACIS)},
  title={On Using Sum-Product Networks for Multi-label Classification},
  year={2017},
  volume={},
  number={},
  pages={25-30},
  doi={10.1109/BRACIS.2017.34}
}

@INPROCEEDINGS{dennis17,
  author={Dennis, Aaron and Ventura, Dan},
  booktitle={2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)},
  title={Autoencoder-Enhanced Sum-Product Networks},
  year={2017},
  volume={},
  number={},
  pages={1041-1044},
  doi={10.1109/ICMLA.2017.00-13}
}

@inproceedings{friesen17,
  title={Unifying sum-product networks and submodular fields},
  author={Friesen, Abram L and Domingos, Pedro},
  booktitle={Proceedings of the Workshop on Principled Approaches to Deep Learning at ICML},
  year={2017}
}

@article{yuan16,
author = {Yuan, Zehuan and Wang, Hao and Wang, Limin and Lu, Tong and Palaiahnakote, Shivakumara and Lim Tan, Chew},
title = {Modeling Spatial Layout for Scene Image Understanding via a Novel Multiscale Sum-Product Network},
year = {2016},
issue_date = {November 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {63},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2016.07.015},
doi = {10.1016/j.eswa.2016.07.015},
abstract = {A new deep architecture MSPN is proposed for image segmentation.Multiscale unary potentials are used to model image spatial layouts.A superpixel-based refinement method is used to improve the parsing results. Semantic image segmentation is challenging due to the large intra-class variations and the complex spatial layouts inside natural scenes. This paper investigates this problem by designing a new deep architecture, called multiscale sum-product network (MSPN), which utilizes multiscale unary potentials as the inputs and models the spatial layouts of image content in a hierarchical manner. That is, the proposed MSPN models the joint distribution of multiscale unary potentials and object classes instead of single unary potentials in popular settings. Besides, MSPN characterizes scene spatial layouts in a fine-to-coarse manner to enforce the consistency in labeling. Multiscale unary potentials at different scales can thus help overcome semantic ambiguities caused by only evaluating single local regions, while long-range spatial correlations can further refine image labeling. In addition, higher orders are able to pose the constraints among labels. By this way, multi-scale unary potentials, long-range spatial correlations, higher-order priors are well modeled under the uniform framework in MSPN. We conduct experiments on two challenging benchmarks consisting of the MSRC-21 dataset and the SIFT FLOW dataset. The results demonstrate the superior performance of our method comparing with the previous graphical models for understanding scene images.},
journal = {Expert Syst. Appl.},
month = {nov},
pages = {231–240},
numpages = {10},
keywords = {Multiscale unary potentials, Multiscale sum-product network, MSPN, Scene image understanding, Spatial layout}
}

@inproceedings{rathke17,
  title={Locally adaptive probabilistic models for global segmentation of pathological OCT scans},
  author={Rathke, Fabian and Desana, Mattia and Schn{\"o}rr, Christoph},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={177--184},
  year={2017},
  organization={Springer}
}

@ARTICLE{wang18,
  author={Wang, Jinghua and Wang, Gang},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  title={Hierarchical Spatial Sum–Product Networks for Action Recognition in Still Images},
  year={2018},
  volume={28},
  number={1},
  pages={90-100},
  doi={10.1109/TCSVT.2016.2586853}
}


@INPROCEEDINGS{amer12,
  author={Amer, Mohamed R. and Todorovic, Sinisa},
  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition},
  title={Sum-product networks for modeling activities with stochastic structure},
  year={2012},
  volume={},
  number={},
  pages={1314-1321},
  doi={10.1109/CVPR.2012.6247816}
}

@inproceedings{friesen18,
 author = {Friesen, Abram L and Domingos, Pedro M},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Submodular Field Grammars: Representation, Inference, and Application to Image Parsing},
 url = {https://proceedings.neurips.cc/paper/2018/file/c5866e93cab1776890fe343c9e7063fb-Paper.pdf},
 volume = {31},
 year = {2018}
}

@misc{nourani20,
      title={Don't Explain without Verifying Veracity: An Evaluation of Explainable AI with Video Activity Recognition},
      author={Mahsan Nourani and Chiradeep Roy and Tahrima Rahman and Eric D. Ragan and Nicholas Ruozzi and Vibhav Gogate},
      year={2020},
      eprint={2005.02335},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}


@ARTICLE{amer16,
  author={Amer, Mohamed R. and Todorovic, Sinisa},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title={Sum Product Networks for Activity Recognition},
  year={2016},
  volume={38},
  number={4},
  pages={800-813},
  doi={10.1109/TPAMI.2015.2465955}
}

@inproceedings{peharz14b,
  title={Modeling speech with sum-product networks: Application to bandwidth extension},
  author={Peharz, Robert and Kapeller, Georg and Mowlaee, Pejman and Pernkopf, Franz},
  booktitle={2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3699--3703},
  year={2014},
  organization={IEEE}
}

@inproceedings{ratajczak14,
  title={Sum-product networks for structured prediction: Context-specific deep conditional random fields},
  author={Ratajczak, Martin and Tschiatschek, Sebastian and Pernkopf, Franz},
  booktitle={International Conference on Machine Learning (ICML) Workshop on Learning Tractable Probabilistic Models Workshop},
  year={2014}
}

@inproceedings{melibari16b,
  author = {Melibari, Mazen and Poupart, Pascal and Doshi, Prashant and Trimponias, George},
  title = {Dynamic Sum Product Networks for Tractable Inference on Sequence Data},
  booktitle = {Probabilistic Graphical Models},
  series = {{JMLR} Workshop and Conference Proceedings},
  volume = {52},
  pages = {345--355},
  publisher = {JMLR.org},
  year = {2016}
}

@inproceedings{pronobis17,
  title={Deep spatial affordance hierarchy: Spatial knowledge representation for planning in large-scale environments},
  author={Pronobis, Andrzej and Riccio, Francesco and Rao, Rajesh PN},
  booktitle={ICAPS 2017 Workshop on Planning and Robotics},
  year={2017}
}

@inproceedings{saad21,
  title = {{SPPL:} Probabilistic Programming with Fast Exact Symbolic Inference},
  author = {Saad, Feras A. and Rinard, Martin C. and Mansinghka, Vikash K.},
  booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
  pages = {804--819},
  numpages = {16},
  year = {2021},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3453483.3454078},
  isbn = {9781450383912},
  keywords = {probabilistic programming, symbolic execution},
  location = {Virtual, Canada},
}

@article{stuhlmuller12,
  title={A dynamic programming algorithm for inference in recursive probabilistic programs},
  author={Stuhlm{\"u}ller, Andreas and Goodman, Noah D},
  journal={Workshop of Statistical and Relational AI (StarAI)},
  year={2012}
}

@inproceedings{shah21,
  author= {Shah, Nimish and Olascoaga, Laura Isabel Galindez and Zhao, Shirui and Meert, Wannes and Verhelst, Marian},
  booktitle={2021 IEEE International Solid- State Circuits Conference (ISSCC)},
  title={PIU: A 248GOPS/W Stream-Based Processor for Irregular Probabilistic Inference Networks Using Precision-Scalable Posit Arithmetic in 28nm},
  year={2021},
  volume={64},
  number={},
  pages={150-152},
  doi={10.1109/ISSCC42613.2021.9366061}
}

@inproceedings{shah20,
author = {Shah, Nimish and Olascoaga, Laura Isabel Galindez and Meert, Wannes and Verhelst, Marian},
title = {Acceleration of probabilistic reasoning through custom processor architecture},
booktitle = {{DATE}},
pages = {322--325},
publisher = {{IEEE}},
year = {2020}
}

@inproceedings{olascoaga19,
author = {Olascoaga, Laura Isabel Galindez and Meert, Wannes and Shah, Nimish and Verhelst, Marian and Van den Broeck, Guy},
title = {Towards Hardware-Aware Tractable Learning of Probabilistic Models},
booktitle = {NeurIPS},
pages = {13726--13736},
year = {2019}
}

@inproceedings{shah19,
author = {Shah, Nimish and Isabel Galindez Olascoaga, Laura and Meert, Wannes and Verhelst, Marian},
title = {ProbLP: A framework for low-precision probabilistic inference},
booktitle = {DAC 2019},
pages = {190},
publisher = {ACM},
year = {2019},
url = {https://doi.org/10.1145/3316781.3317885},
doi = {10.1145/3316781.3317885}
}

@inproceedings{sommer18,
author = {Sommer, Lukas and Oppermann, Julian and Molina, Alejandro and Binnig, Carsten and Kersting, Kristian and Koch, Andreas},
title = {Automatic Mapping of the Sum-Product Network Inference Problem to FPGA-Based Accelerators},
booktitle = {{ICCD}},
pages = {350--357},
publisher = {{IEEE} Computer Society},
year = {2018}
}

@article{hartigan79,
 ISSN = {00359254, 14679876},
 URL = {http://www.jstor.org/stable/2346830},
 author = {J. A. Hartigan and M. A. Wong},
 journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
 number = {1},
 pages = {100--108},
 publisher = {[Wiley, Royal Statistical Society]},
 title = {Algorithm AS 136: A K-Means Clustering Algorithm},
 volume = {28},
 year = {1979}
}

@article{bergstra12a,
  author  = {James Bergstra and Yoshua Bengio},
  title   = {Random Search for Hyper-Parameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {10},
  pages   = {281-305},
  url     = {http://jmlr.org/papers/v13/bergstra12a.html}
}

@InProceedings{lowd13a,
  title = 	 {Learning Markov Networks With Arithmetic Circuits},
  author = 	 {Lowd, Daniel and Rooshenas, Amirmohammad},
  booktitle = 	 {Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {406--414},
  year = 	 {2013},
  editor = 	 {Carvalho, Carlos M. and Ravikumar, Pradeep},
  volume = 	 {31},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Scottsdale, Arizona, USA},
  month = 	 apr,
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v31/lowd13a.pdf},
  url = 	 {https://proceedings.mlr.press/v31/lowd13a.html},
  abstract = 	 {Markov networks are an effective way to represent complex probability distributions.  However, learning their structure and parameters or using them to answer queries is typically intractable.  One approach to making learning and inference tractable is to use approximations, such as pseudo-likelihood or approximate inference.  An alternate approach is to use a restricted class of models where exact inference is always efficient.  Previous work has explored low treewidth models, models with tree-structured features, and latent variable models.  In this paper, we introduce ACMN, the first ever method for learning efficient Markov networks with arbitrary conjunctive features.  The secret to ACMN’s greater flexibility is its use of arithmetic circuits, a linear-time inference representation that can handle many high treewidth models by exploiting local structure.  ACMN uses the size of the corresponding arithmetic circuit as a learning bias, allowing it to trade off accuracy and inference complexity.  In experiments on 12 standard datasets, the tractable models learned by ACMN are more accurate than both tractable models learned by other algorithms and approximate inference in intractable models. }
}

@article{feldmann15,
author = {Feldmann, Andreas Emil and Foschini, Luca},
title = {Balanced Partitions of Trees and Applications},
year = {2015},
issue_date = {February  2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {71},
number = {2},
issn = {0178-4617},
url = {https://doi.org/10.1007/s00453-013-9802-3},
doi = {10.1007/s00453-013-9802-3},
abstract = {We study the problem of finding the minimum number of edges that, when cut, form a partition of the vertices into k sets of equal size. This is called the k- BALANCED PARTITIONING problem. The problem is known to be inapproximable within any finite factor on general graphs, while little is known about restricted graph classes. We show that the k- BALANCED PARTITIONING problem remains APX-hard even when restricted to unweighted tree instances with constant maximum degree. If instead the diameter of the tree is constant we prove that the problem is NP-hard to approximate within n c , for any constant c&lt;1. If vertex sets are allowed to deviate from being equal-sized by a factor of at most 1+ , we show that solutions can be computed on weighted trees with cut cost no worse than the minimum attainable when requiring equal-sized sets. This result is then extended to general graphs via decompositions into trees and improves the previously best approximation ratio from O(log1.5(n) 2) [Andreev and Rcke in Theory Comput. Syst. 39(6):929---939, 2006 ] to O(logn). This also settles the open problem of whether an algorithm exists for which the number of edges cut is independent of .},
journal = {Algorithmica},
month = {feb},
pages = {354–376},
numpages = {23}
}

@article{liu19,
title = {The optimization of sum-product network structure learning},
journal = {Journal of Visual Communication and Image Representation},
volume = {60},
pages = {391-397},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300653},
author = {Yang Liu and Tiejian Luo},
keywords = {Machine learning, Deep learning, Sum-product network, Structure learning},
abstract = {Sum-Product Network (SPN) are recently introduced deep tractable Probabilistic Graphical Models providing exact and tractable inference. SPN have been successfully employed as density estimators in some artificial intelligence fields, however, most of the proposed structure learning algorithms focus on improving the performance of a certain aspect of model, at the cost of reducing other performance. This is due to the fact that there is no effective balance between network width and depth during learning process. In this paper, we propose two clustering analysis algorithms to replace the clustering part of LearnSPN. We improve the structure quality of the generated model by deepening the network while adjusting the network width adaptively, trying to find a balance between the expressive power, representation ability, inference accuracy and simplicity. Experimental results prove that LearnSPN equipped by our clustering method has different degrees of improvement in various performances.}
}

@article{molina17,
  title={Poisson Sum-Product Networks: A Deep Architecture for Tractable Multivariate Poisson Distributions},
  volume={31},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/10844},
  abstractNote={Multivariate count data are pervasive in science in the form of histograms, contingency tables and others. Previous work on modeling this type of distributions do not allow for fast and tractable inference. In this paper we present a novel Poisson graphical model, the first based on sum product networks, called PSPN, allowing for positive as well as negative dependencies. We present algorithms for learning tree PSPNs from data as well as for tractable inference via symbolic evaluation. With these, information-theoretic measures such as entropy, mutual information, and distances among count variables can be computed without resorting to approximations. Additionally, we show a connection between PSPNs and LDA, linking the structure of tree PSPNs to a hierarchy of topics. The experimental results on several synthetic and real world datasets demonstrate that PSPN often outperform state-of-the-art while remaining tractable.},
  number={1},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Molina, Alejandro and Natarajan, Sriraam and Kersting, Kristian},
  year={2017},
  month={Feb.}
}

@article{bueff18,
  author    = {Andreas Bueff and
               Stefanie Speichert and
               Vaishak Belle},
  title     = {Tractable Querying and Learning in Hybrid Domains via Sum-Product
               Networks},
  journal   = {CoRR},
  volume    = {abs/1807.05464},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.05464},
  eprinttype = {arXiv},
  eprint    = {1807.05464},
  timestamp = {Mon, 13 Aug 2018 16:47:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1807-05464.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lopez13,
 author = {Lopez-Paz, David and Hennig, Philipp and Sch\"{o}lkopf, Bernhard},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {The Randomized Dependence Coefficient},
 url = {https://proceedings.neurips.cc/paper/2013/file/aab3238922bcc25a6f606eb525ffdc56-Paper.pdf},
 volume = {26},
 year = {2013}
}

@article{molina18,
	author = {Alejandro Molina and Antonio Vergari and Nicola Di Mauro and Sriraam Natarajan and Floriana Esposito and Kristian Kersting},
	title = {Mixed Sum-Product Networks: A Deep Architecture for Hybrid Domains},
	conference = {AAAI Conference on Artificial Intelligence},
	year = {2018},
	keywords = {sum-product networks, non-parametric density estimation, non-parameteric independency test, hybrid domains, mixed graphical models},
	abstract = {While all kinds of mixed data---from personal data, over panel and scientific data, to public and commercial data---are collected and stored, building probabilistic graphical models for these hybrid domains becomes more difficult. Users spend significant amounts of time in identifying the parametric form of the random variables (Gaussian, Poisson, Logit, etc.) involved and learning the mixed models. To make this difficult task easier,  we propose the first trainable probabilistic deep architecture for hybrid domains that features tractable queries. It is based on Sum-Product Networks (SPNs) with piecewise polynomial leaf distributions together with novel nonparametric decomposition and conditioning steps using the Hirschfeld-Gebelein-Renyi Maximum Correlation Coefficient. This relieves the user from deciding a-priori the parametric form of the random variables but is still expressive enough to effectively approximate any distribution and permits efficient learning and inference.Our experiments show that the architecture, called Mixed SPNs,  can indeed capture complex distributions across a wide range of hybrid domains.},

	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16865}
}

@article{nath15,
  title={Learning Relational Sum-Product Networks},
  volume={29},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/9538},
  abstract={Sum-product networks (SPNs) are a recently-proposed deep architecture that guarantees tractable inference, even on certain high-treewidth models. SPNs are a propositional architecture, treating the instances as independent and identically distributed. In this paper, we introduce Relational Sum-Product Networks (RSPNs), a new tractable first-order probabilistic architecture. RSPNs generalize SPNs by modeling a set of instances jointly, allowing them to influence each other’s probability distributions, as well as modeling probabilities of relations between objects. We also present LearnRSPN, the first algorithm for learning high-treewidth tractable statistical relational models. LearnRSPN is a recursive top-down structure learning algorithm for RSPNs, based on Gens and Domingos’ LearnSPN algorithm for propositional SPN learning. We evaluate the algorithm on three datasets; the RSPN learning algorithm outperforms Markov Logic Networks in both running time and predictive accuracy.},
  number={1},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Nath, Aniruddh and Domingos, Pedro},
  year={2015},
  month={Feb.}
}

@InProceedings{butz18a,
  title = 	 {An Empirical Study of Methods for SPN Learning and Inference},
  author =       {Butz, Cory J. and Oliveira, Jhonatan S. and dos Santos, Andr\'{e} E. and Teixeira, Andr\'{e} L. and Poupart, Pascal and Kalra, Agastya},
  booktitle = 	 {Proceedings of the Ninth International Conference on Probabilistic Graphical Models},
  pages = 	 {49--60},
  year = 	 {2018},
  editor = 	 {Kratochvíl, Václav and Studený, Milan},
  volume = 	 {72},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {11--14 Sep},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v72/butz18a/butz18a.pdf},
  url = 	 {https://proceedings.mlr.press/v72/butz18a.html},
  abstract = 	 {In this study, we provide an empirical comparison of methods for \emph{sum-product network} (SPN) learning and inference. LearnSPN is a popular algorithm for learning SPNs that utilizes chop and slice operations. As \emph{g-test} is a standard chopping method and \emph{Gaussian mixture models} (GMM) using expectation-maximization is a common slicing method, it seems to have been assumed in the literature that this is the best pair in LearnSPN. On the contrary, our results show that g-test for chopping and \emph{k-means} for slicing yields SPNs that are just as accurate. Moreover, it has been shown that implementing SPN leaf nodes as \emph{Chow-Liu Trees} (CLTs) yields more accurate SPNs for the former pair. Our experiments show the same for the latter pair, and that neither pair dominates the other. Lastly, we report an analysis of SPN topology for unstudied pairs. With respect to inference, we derive \emph{partial propagation} (PP), which performs SPN exact inference without requiring a full propagation over all nodes in the SPN as currently done. Experimental results on SPN datasets demonstrate that PP has several advantages over full propagation in SPNs, including relative time savings, absolute time savings in large SPNs, and scalability.}
}

@inproceedings{tahrima16,
  author = {Rahman, Tahrima and Gogate, Vibhav},
  title = {Merging Strategies for Sum-Product Networks: From Trees to Graphs},
  year = {2016},
  isbn = {9780996643115},
  publisher = {AUAI Press},
  address = {Arlington, Virginia, USA},
  abstract = {Learning the structure of sum-product networks (SPNs) - arithmetic circuits over latent and observed variables - has been the subject of much recent research. These networks admit linear time exact inference, and thus help alleviate one of the chief disadvantages of probabilistic graphical models: accurate probabilistic inference algorithms are often computationally expensive. Although, algorithms for inducing their structure from data have come quite far and often outperform algorithms that induce probabilistic graphical models, a key issue with existing approaches is that they induce tree SPNs, a small, inefficient sub-class of SPNs. In this paper, we address this limitation by developing post-processing approaches that induce graph SPNs from tree SPNs by merging similar sub-structures. The key benefits of graph SPNs over tree SPNs include smaller computational complexity which facilitates faster online inference, and better generalization accuracy because of reduced variance, at the cost of slight increase in the learning time. We demonstrate experimentally that our merging techniques significantly improve the accuracy of tree SPNs, achieving state-of-the-art performance on several real world benchmark datasets.},
  booktitle = {Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence},
  pages = {617–626},
  numpages = {10},
  location = {Jersey City, New Jersey, USA},
  series = {UAI'16}
}

@book{garey90,
author = {Garey, Michael R. and Johnson, David S.},
title = {Computers and Intractability; A Guide to the Theory of NP-Completeness},
year = {1990},
isbn = {0716710455},
publisher = {W. H. Freeman Co.},
address = {USA}
}

@article{karypsis98,
  author = {Karypis, George and Kumar, Vipin},
  title = {A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs},
  journal = {SIAM Journal on Scientific Computing},
  volume = {20},
  number = {1},
  pages = {359-392},
  year = {1998},
  doi = {10.1137/S1064827595287997},
}

@article{kolmogorov09,
  author={Kolmogorov, Vladimir},
  title={Blossom V: a new implementation of a minimum cost perfect matching algorithm},
  journal={Mathematical Programming Computation},
  year={2009},
  month={Jul},
  day={01},
  volume={1},
  number={1},
  pages={43-67},
  abstract={We describe a new implementation of the Edmonds's algorithm for computing a perfect matching of minimum cost, to which we refer as Blossom V. A key feature of our implementation is a combination of two ideas that were shown to be effective for this problem: the ``variable dual updates'' approach of Cook and Rohe (INFORMS J Comput 11(2):138--148, 1999) and the use of priority queues. We achieve this by maintaining an auxiliary graph whose nodes correspond to alternating trees in the Edmonds's algorithm. While our use of priority queues does not improve the worst-case complexity, it appears to lead to an efficient technique. In the majority of our tests Blossom V outperformed previous implementations of Cook and Rohe (INFORMS J Comput 11(2):138--148, 1999) and Mehlhorn and Sch{\"a}fer (J Algorithmics Exp (JEA) 7:4, 2002), sometimes by an order of magnitude. We also show that for large VLSI instances it is beneficial to update duals by solving a linear program, contrary to a conjecture by Cook and Rohe.},
  issn={1867-2957},
  doi={10.1007/s12532-009-0002-8},
  url={https://doi.org/10.1007/s12532-009-0002-8}
}

@article{edmonds65,
  title={Paths, Trees, and Flowers},
  volume={17},
  DOI={10.4153/CJM-1965-045-4},
  journal={Canadian Journal of Mathematics},
  publisher={Cambridge University Press},
  author={Edmonds, Jack},
  year={1965},
  pages={449–467}
}


@ARTICLE{chow68,
  author={Chow, C. and Liu, C.},
  journal={IEEE Transactions on Information Theory},
  title={Approximating discrete probability distributions with dependence trees},
  year={1968},
  volume={14},
  number={3},
  pages={462-467},
  doi={10.1109/TIT.1968.1054142}
}

@inproceedings{boutilier96,
author = {Boutilier, Craig and Friedman, Nir and Goldszmidt, Moises and Koller, Daphne},
title = {Context-Specific Independence in Bayesian Networks},
year = {1996},
isbn = {155860412X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Bayesian networks provide a language for qualitatively representing the conditional independence properties of a distribution, This allows a natural and compact representation of the distribution, eases knowledge acquisition, and supports effective inference algorithms. It is well-known, however, that there are certain independencies that we cannot capture qualitatively within the Bayesian network structure: independencies that hold only in certain contexts, i.e., given a specific assignment of values to certain variables, In this paper, we propose a formal notion of context-specific independence (CSI), based on regularities in the conditional probability tables (CPTs) at a node. We present a technique, analogous to (and based on) d-separation, for determining when such independence holds in a given network. We then focus on a particular qualitative representation scheme--tree-structured CPTs-- for capturing CSI. We suggest ways in which this representation can be used to support effective inference algorithms, in particular, we present a structural decomposition of the resulting network which can improve the performance of clustering algorithms, and an alternative algorithm based on outset conditioning.},
booktitle = {Proceedings of the Twelfth International Conference on Uncertainty in Artificial Intelligence},
pages = {115–123},
numpages = {9},
location = {Portland, OR},
series = {UAI'96}
}

@article{vlasselaer15,
	author = {Jonas Vlasselaer and Guy Van den Broeck and Angelika Kimmig and Wannes Meert and Luc De Raedt},
	title = {Anytime Inference in Probabilistic Logic Programs with Tp-Compilation},
	conference = {International Joint Conference on Artificial Intelligence},
	year = {2015},
	keywords = {},
	abstract = {Existing techniques for inference in probabilistic logic programs are sequential: they first compute the relevant propositional formula for the query of interest, then compile it into a tractable target representation and finally, perform weighted model counting on the resulting representation. We propose Tp-compilation, a new inference technique based on forward reasoning. Tp-compilation proceeds incrementally in that it interleaves the knowledge compilation step for weighted model counting with forward reasoning on the logic program. This leads to a novel anytime algorithm that provides hard bounds on the inferred probabilities. Furthermore, an empirical evaluation shows that Tp-compilation effectively handles larger instances of complex real-world problems than current sequential approaches, both for exact and for anytime approximate inference.},
	url = {https://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/view/11005}
}

@inproceedings{vlasselaer14,
  abstract = {Knowledge compilation algorithms transform a probabilistic logic program into a circuit representation that permits efficient proba- bility computation. Knowledge compilation underlies algorithms for ex- act probabilistic inference and parameter learning in several languages, including ProbLog, PRISM, and LPADs. Developing such algorithms involves a choice, of which circuit language to target, and which compi- lation algorithm to use. Historically, Binary Decision Diagrams (BDDs) have been a popular target language, whereas recently, deterministic- Decomposable Negation Normal Form (d-DNNF) circuits were shown to outperform BDDs on these tasks. We investigate the use of a new language, called Sentential Decision Diagrams (SDDs), for inference in probabilistic logic programs. SDDs combine desirable properties of BDDs and d-DNNFs. Like BDDs, they support bottom-up compilation and cir- cuit minimization, yet they are a more general and flexible representa- tion. Our preliminary experiments show that compilation to SDD yields smaller circuits and more scalable inference, outperforming the state of the art in ProbLog inference.},
  journal = {Proceedings Workshop on Probabilistic Logic Programming (PLP)},
  pages = {1--10},
  year = {2014},
  title = {Compiling probabilistic logic programs into sentential decision diagrams},
  language = {eng},
  author = {Vlasselaer, Jonas and Renkens, Joris and Van den Broeck, Guy and De Raedt, Luc},
  keywords = {knowledge compilation},
}

@inproceedings{lomuscio15,
  author = {Lomuscio, Alessio and Paquet, Hugo},
  title = {Verification of Multi-Agent Systems via SDD-Based Model Checking},
  year = {2015},
  isbn = {9781450334136},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  address = {Richland, SC},
  abstract = {Considerable progress has been achieved in the past ten years in the symbolic verification of Multi-Agent Systems (MAS). One of the most efficient techniques put forward is based on the use of ordered binary decision diagrams (OBDDs) for representing the state space and computing the states at which specifications hold. Sentential Decision Diagrams (SDDs) have recently been put forward as an alternative symbolic representation for Boolean formulas in knowledge representation. In this abstract we report some preliminary results on the applicability of SDDs for the verification of MAS.},
  booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
  pages = {1713–1714},
  numpages = {2},
  keywords = {model-checking, epistemic logic, sentential decision diagrams},
  location = {Istanbul, Turkey},
  series = {AAMAS '15}
}

@INPROCEEDINGS{herrmann13,
  author={Herrmann, Ricardo G. and Barros, Leliane N. de},
  booktitle={2013 Brazilian Conference on Intelligent Systems},
  title={Algebraic Sentential Decision Diagrams in Symbolic Probabilistic Planning},
  year={2013},
  volume={},
  number={},
  pages={175-181},
  doi={10.1109/BRACIS.2013.37}
}

@inproceedings{bova16,
author = {Bova, Simone},
title = {SDDs Are Exponentially More Succinct than OBDDs},
year = {2016},
publisher = {AAAI Press},
abstract = {Introduced by Darwiche (2011), sentential decision diagrams (SDDs) are essentially as tractable as ordered binary decision diagrams (OBDDs), but tend to be more succinct in practice. This makes SDDs a prominent representation language, with many applications in artificial intelligence and knowledge compilation.We prove that SDDs are more succinct than OBDDs also in theory, by constructing a family of boolean functions where each member has polynomial SDD size but exponential OBDD size. This exponential separation improves a quasipolynomial separation recently established by Razgon (2014a), and settles an open problem in knowledge compilation (Darwiche 2011).},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {929–935},
numpages = {7},
location = {Phoenix, Arizona},
series = {AAAI'16}
}


@article{dempster77,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2984875},
 abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
 author = {A. P. Dempster and N. M. Laird and D. B. Rubin},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {1--38},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Maximum Likelihood from Incomplete Data via the EM Algorithm},
 volume = {39},
 year = {1977}
}

@inproceedings{bouchard04,
  TITLE = {{The Tradeoff Between Generative and Discriminative Classifiers}},
  AUTHOR = {Bouchard, Guillaume and Triggs, Bill},
  URL = {https://hal.inria.fr/inria-00548546},
  BOOKTITLE = {{16th IASC International Symposium on Computational Statistics (COMPSTAT '04)}},
  ADDRESS = {Prague, Czech Republic},
  PAGES = {721--728},
  YEAR = {2004},
  MONTH = Aug,
  KEYWORDS = {Statistical computing ; numerical algorithms},
  PDF = {https://hal.inria.fr/inria-00548546/file/Bouchard-compstat04.pdf},
  HAL_ID = {inria-00548546},
  HAL_VERSION = {v1},
}

@ARTICLE{gopalakrishnan91,
  author={Gopalakrishnan, P.S. and Kanevsky, D. and Nadas, A. and Nahamoo, D.},
  journal={IEEE Transactions on Information Theory},
  title={An inequality for rational functions with applications to some statistical estimation problems},
  year={1991},
  volume={37},
  number={1},
  pages={107-113},
  doi={10.1109/18.61108}
}

@misc{sharir18,
      title={Tensorial Mixture Models},
      author={Or Sharir and Ronen Tamari and Nadav Cohen and Amnon Shashua},
      year={2018},
      eprint={1610.04167},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{jaini16,
  title = 	 {Online Algorithms for Sum-Product Networks with Continuous Variables},
  author = 	 {Jaini, Priyank and Rashwan, Abdullah and Zhao, Han and Liu, Yue and Banijamali, Ershad and Chen, Zhitang and Poupart, Pascal},
  booktitle = 	 {Proceedings of the Eighth International Conference on Probabilistic Graphical Models},
  pages = 	 {228--239},
  year = 	 {2016},
  editor = 	 {Antonucci, Alessandro and Corani, Giorgio and Campos, Cassio Polpo},
  volume = 	 {52},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lugano, Switzerland},
  month = 	 {06--09 Sep},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v52/jaini16.pdf},
  url = 	 {https://proceedings.mlr.press/v52/jaini16.html},
  abstract = 	 {Sum-product networks (SPNs) have recently emerged as an attractive representation due to their dual interpretation as a special type of deep neural network with clear semantics and a tractable probabilistic graphical model. We explore online algorithms for parameter learning in SPNs with continuous variables. More specifically, we consider SPNs with Gaussian leaf distributions and show how to derive an online Bayesian moment matching algorithm to learn from streaming data. We compare the resulting generative models to stacked restricted Boltzmann machines and generative moment matching networks on real-world datasets.}
}

@article{vergari19,
  title={Automatic Bayesian Density Analysis},
  volume={33},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/4977},
  DOI={10.1609/aaai.v33i01.33015207},
  abstract={Making sense of a dataset in an automatic and unsupervised fashion is a challenging problem in statistics and AI. Classical approaches for exploratory data analysis are usually not flexible enough to deal with the uncertainty inherent to real-world data: they are often restricted to fixed latent interaction models and homogeneous likelihoods; they are sensitive to missing, corrupt and anomalous data; moreover, their expressiveness generally comes at the price of intractable inference. As a result, supervision from statisticians is usually needed to find the right model for the data. However, since domain experts are not necessarily also experts in statistics, we propose Automatic Bayesian Density Analysis (ABDA) to make exploratory data analysis accessible at large. Specifically, ABDA allows for automatic and efficient missing value estimation, statistical data type and likelihood discovery, anomaly detection and dependency structure mining, on top of providing accurate density estimation. Extensive empirical evidence shows that ABDA is a suitable tool for automatic exploratory analysis of mixed continuous and discrete tabular data.},
  number={01},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Vergari, Antonio and Molina, Alejandro and Peharz, Robert and Ghahramani, Zoubin and Kersting, Kristian and Valera, Isabel},
  year={2019},
  month=jul,
  pages={5207-5215}
}

@article{mattei20b,
title = {Tractable inference in credal sentential decision diagrams},
journal = {International Journal of Approximate Reasoning},
volume = {125},
pages = {26-48},
year = {2020},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2020.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X20301845},
author = {Lilith Mattei and Alessandro Antonucci and Denis Deratani Mauá and Alessandro Facchini and Julissa {Villanueva Llerena}},
keywords = {Probabilistic graphical models, Imprecise probability, Credal sets, Probabilistic circuits, Sentential decision diagrams, Sum-product networks},
abstract = {Probabilistic sentential decision diagrams are logic circuits where the inputs of disjunctive gates are annotated by probability values. They allow for a compact representation of joint probability mass functions defined over sets of Boolean variables, that are also consistent with the logical constraints defined by the circuit. The probabilities in such a model are usually “learned” from a set of observations. This leads to overconfident and prior-dependent inferences when data are scarce, unreliable or conflicting. In this work, we develop the credal sentential decision diagrams, a generalisation of their probabilistic counterpart that allows for replacing the local probabilities with (so-called credal) sets of mass functions. These models induce a joint credal set over the set of Boolean variables, that sharply assigns probability zero to states inconsistent with the logical constraints. Three inference algorithms are derived for these models. These allow to compute: (i) the lower and upper probabilities of an observation for an arbitrary number of variables; (ii) the lower and upper conditional probabilities for the state of a single variable given an observation; (iii) whether or not all the probabilistic sentential decision diagrams compatible with the credal specification have the same most probable explanation of a given set of variables given an observation of the other variables. These inferences are tractable, as all the three algorithms, based on bottom-up traversal with local linear programming tasks on the disjunctive gates, can be solved in polynomial time with respect to the circuit size. The first algorithm is always exact, while the remaining two might induce a conservative (outer) approximation in the case of multiply connected circuits. A semantics for this approximation together with an auxiliary algorithm able to decide whether or not the result is exact is also provided together with a brute-force characterization of the exact inference in these cases. For a first empirical validation, we consider a simple application based on noisy seven-segment display images. The credal models are observed to properly distinguish between easy and hard-to-detect instances and outperform other generative models not able to cope with logical constraints.}
}

@INPROCEEDINGS{ho95,
  author={Tin Kam Ho},
  booktitle={Proceedings of 3rd International Conference on Document Analysis and Recognition},
  title={Random decision forests},
  year={1995},
  volume={1},
  number={},
  pages={278-282 vol.1},
  doi={10.1109/ICDAR.1995.598994}
}

@misc{hang19,
      title={Best-scored Random Forest Density Estimation},
      author={Hanyuan Hang and Hongwei Wen},
      year={2019},
      eprint={1905.03729},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{ram11,
author = {Ram, Parikshit and Gray, Alexander G.},
title = {Density Estimation Trees},
year = {2011},
isbn = {9781450308137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020408.2020507},
doi = {10.1145/2020408.2020507},
abstract = {In this paper we develop density estimation trees (DETs), the natural analog of classification trees and regression trees, for the task of density estimation. We consider the estimation of a joint probability density function of a d-dimensional random vector X and define a piecewise constant estimator structured as a decision tree. The integrated squared error is minimized to learn the tree. We show that the method is nonparametric: under standard conditions of nonparametric density estimation, DETs are shown to be asymptotically consistent. In addition, being decision trees, DETs perform automatic feature selection. They empirically exhibit the interpretability, adaptability and feature selection properties of supervised decision trees while incurring slight loss in accuracy over other nonparametric density estimators. Hence they might be able to avoid the curse of dimensionality if the true density is sparse in dimensions. We believe that density estimation trees provide a new tool for exploratory data analysis with unique capabilities.},
booktitle = {Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {627–635},
numpages = {9},
keywords = {density estimation, decision trees},
location = {San Diego, California, USA},
series = {KDD '11}
}

@inproceedings{smyth95,
  author    = {Padhraic Smyth and
               Alexander G. Gray and
               Usama M. Fayyad},
  editor    = {Armand Prieditis and
               Stuart J. Russell},
  title     = {Retrofitting Decision Tree Classifiers Using Kernel Density Estimation},
  booktitle = {Machine Learning, Proceedings of the Twelfth International Conference
               on Machine Learning, Tahoe City, California, USA, July 9-12, 1995},
  pages     = {506--514},
  publisher = {Morgan Kaufmann},
  year      = {1995},
  url       = {https://doi.org/10.1016/b978-1-55860-377-6.50069-4},
  doi       = {10.1016/b978-1-55860-377-6.50069-4},
  timestamp = {Mon, 24 Jun 2019 15:47:45 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/SmythGF95.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bentley75,
author = {Bentley, Jon Louis},
title = {Multidimensional Binary Search Trees Used for Associative Searching},
year = {1975},
issue_date = {Sept. 1975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/361002.361007},
doi = {10.1145/361002.361007},
abstract = {This paper develops the multidimensional binary search tree (or k-d tree, where k is the dimensionality of the search space) as a data structure for storage of information to be retrieved by associative searches. The k-d tree is defined and examples are given. It is shown to be quite efficient in its storage requirements. A significant advantage of this structure is that a single data structure can handle many types of queries very efficiently. Various utility algorithms are developed; their proven average running times in an n record file are: insertion, O(log n); deletion of the root, O(n(k-1)/k); deletion of a random node, O(log n); and optimization (guarantees logarithmic performance of searches), O(n log n). Search algorithms are given for partial match queries with t keys specified [proven maximum running time of O(n(k-t)/k)] and for nearest neighbor queries [empirically observed average running time of O(log n).] These performances far surpass the best currently known algorithms for these tasks. An algorithm is presented to handle any general intersection query. The main focus of this paper is theoretical. It is felt, however, that k-d trees could be quite useful in many applications, and examples of potential uses are given.},
journal = {Commun. ACM},
month = {sep},
pages = {509–517},
numpages = {9},
keywords = {binary tree insertion, attribute, intersection queries, binary search trees, associative retrieval, key, partial match queries, nearest neighbor queries, information retrieval system}
}

@inproceedings{spirtes95,
author = {Spirtes, Peter and Meek, Christopher},
title = {Learning Bayesian Networks with Discrete Variables from Data},
year = {1995},
publisher = {AAAI Press},
abstract = {This paper describes a new greedy Bayesian search algorithm (GBPS) and a new "combined" algorithm PC+GBPS for learning Bayesian networks. Simulation tests of these algorithms with previously published algorithms are presented.},
booktitle = {Proceedings of the First International Conference on Knowledge Discovery and Data Mining},
pages = {294–299},
numpages = {6},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {KDD'95}
}

@inproceedings{teyssier05,
author = {Teyssier, Marc and Koller, Daphne},
title = {Ordering-Based Search: A Simple and Effective Algorithm for Learning Bayesian Networks},
year = {2005},
isbn = {0974903914},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {One of the basic tasks for Bayesian networks (BNs) is that of learning a network structure from data. The BN-learning problem is NP-hard, so the standard solution is heuristic search. Many approaches have been proposed for this task, but only a very small number outperform the baseline of greedy hill-climbing with tabu lists; moreover, many of the proposed algorithms are quite complex and hard to implement. In this paper, we propose a very simple and easy-to-implement method for addressing this task. Our approach is based on the well-known fact that the best network (of bounded in-degree) consistent with a given node ordering can be found very efficiently. We therefore propose a search not over the space of structures, but over the space of orderings, selecting for each ordering the best network consistent with it. This search space is much smaller, makes more global search steps, has a lower branching factor, and avoids costly acyclicity checks. We present results for this algorithm on both synthetic and real data sets, evaluating both the score of the network found and in the running time. We show that ordering-based search outperforms the standard baseline, and is competitive with recent algorithms that are much harder to implement.},
booktitle = {Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence},
pages = {584–590},
numpages = {7},
location = {Edinburgh, Scotland},
series = {UAI'05}
}

@inproceedings{cook71,
author = {Cook, Stephen A.},
title = {The Complexity of Theorem-Proving Procedures},
year = {1971},
isbn = {9781450374644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800157.805047},
doi = {10.1145/800157.805047},
abstract = {It is shown that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be “reduced” to the problem of determining whether a given propositional formula is a tautology. Here “reduced” means, roughly speaking, that the first problem can be solved deterministically in polynomial time provided an oracle is available for solving the second. From this notion of reducible, polynomial degrees of difficulty are defined, and it is shown that the problem of determining tautologyhood has the same polynomial degree as the problem of determining whether the first of two given graphs is isomorphic to a subgraph of the second. Other examples are discussed. A method of measuring the complexity of proof procedures for the predicate calculus is introduced and discussed.},
booktitle = {Proceedings of the Third Annual ACM Symposium on Theory of Computing},
pages = {151–158},
numpages = {8},
location = {Shaker Heights, Ohio, USA},
series = {STOC '71}
}

@InProceedings{salakhutdinov09,
  title = 	 {Deep Boltzmann Machines},
  author = 	 {Salakhutdinov, Ruslan and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {448--455},
  year = 	 {2009},
  editor = 	 {van Dyk, David and Welling, Max},
  volume = 	 {5},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
  month = 	 apr,
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf},
  url = 	 {https://proceedings.mlr.press/v5/salakhutdinov09a.html},
  abstract = 	 {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized by a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.}
}

@misc{hsu17,
      title={Online Structure Learning for Sum-Product Networks with Gaussian Leaves},
      author={Wilson Hsu and Agastya Kalra and Pascal Poupart},
      year={2017},
      eprint={1701.05265},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{dua17,
  author = "Dua, Dheeru and Graff, Casey",
  year = "2017",
  title = "{UCI} Machine Learning Repository",
  url = "http://archive.ics.uci.edu/ml",
  institution = "University of California, Irvine, School of Information and Computer Sciences"
}

@misc{guvenir00,
  author = {G{\"u}venir, Halil Altay and Uysal, İlhan},
  year = {2000},
  month = {01},
  pages = {},
  title = {Bilkent University Function Approximation Repository},
  journal = {http://funapp.cs.bilkent.edu.tr}
}

@inproceedings{liu21,
  author 	= {Liu, Anji and Van den Broeck, Guy},
  title     = {Tractable Regularization of Probabilistic Circuits},
  booktitle = {Advances in Neural Information Processing Systems 35 (NeurIPS)},
  month     = {dec},
  year      = {2021},
  url       = "http://starai.cs.ucla.edu/papers/LiuNeurIPS21.pdf",
}

@article{grigorescu20,
author = {Grigorescu, Sorin and Trasnea, Bogdan and Cocias, Tiberiu and Macesanu, Gigel},
title = {A survey of deep learning techniques for autonomous driving},
journal = {Journal of Field Robotics},
volume = {37},
number = {3},
pages = {362-386},
keywords = {AI for self-driving vehicles, artificial intelligence, autonomous driving, deep learning for autonomous driving},
abstract = {Abstract The last decade witnessed increasingly rapid progress in self-driving vehicle technology, mainly backed up by advances in the area of deep learning and artificial intelligence (AI). The objective of this paper is to survey the current state-of-the-art on deep learning technologies used in autonomous driving. We start by presenting AI-based self-driving architectures, convolutional and recurrent neural networks, as well as the deep reinforcement learning paradigm. These methodologies form a base for the surveyed driving scene perception, path planning, behavior arbitration, and motion control algorithms. We investigate both the modular perception-planning-action pipeline, where each module is built using deep learning methods, as well as End2End systems, which directly map sensory information to steering commands. Additionally, we tackle current challenges encountered in designing AI architectures for autonomous driving, such as their safety, training data sources, and computational hardware. The comparison presented in this survey helps gain insight into the strengths and limitations of deep learning and AI approaches for autonomous driving and assist with design choices.},
year = {2020}
}

@Article{lan18,
  author={Lan, Kun and Wang, Dan-tong and Fong, Simon and Liu, Lian-sheng and Wong, Kelvin K. L. and Dey, Nilanjan},
  title={A Survey of Data Mining and Deep Learning in Bioinformatics},
  journal={Journal of Medical Systems},
  year={2018},
  month={Jun},
  day={28},
  volume={42},
  number={8},
  pages={139},
  abstract={The fields of medicine science and health informatics have made great progress recently and have led to in-depth analytics that is demanded by generation, collection and accumulation of massive data. Meanwhile, we are entering a new period where novel technologies are starting to analyze and explore knowledge from tremendous amount of data, bringing limitless potential for information growth. One fact that cannot be ignored is that the techniques of machine learning and deep learning applications play a more significant role in the success of bioinformatics exploration from biological data point of view, and a linkage is emphasized and established to bridge these two data analytics techniques and bioinformatics in both industry and academia. This survey concentrates on the review of recent researches using data mining and deep learning approaches for analyzing the specific domain knowledge of bioinformatics. The authors give a brief but pithy summarization of numerous data mining algorithms used for preprocessing, classification and clustering as well as various optimized neural network architectures in deep learning methods, and their advantages and disadvantages in the practical applications are also discussed and compared in terms of their industrial usage. It is believed that in this review paper, valuable insights are provided for those who are dedicated to start using data analytics methods in bioinformatics.},
  issn={1573-689X},
  doi={10.1007/s10916-018-1003-9},
}

@article{li19,
  title = {Deep learning in bioinformatics: Introduction, application, and perspective in the big data era},
  journal = {Methods},
  volume = {166},
  pages = {4-21},
  year = {2019},
  note = {Deep Learning in Bioinformatics},
  issn = {1046-2023},
  author = {Yu Li and Chao Huang and Lizhong Ding and Zhongxiao Li and Yijie Pan and Xin Gao},
  abstract = {Deep learning, which is especially formidable in handling big data, has achieved great success in various fields, including bioinformatics. With the advances of the big data era in biology, it is foreseeable that deep learning will become increasingly important in the field and will be incorporated in vast majorities of analysis pipelines. In this review, we provide both the exoteric introduction of deep learning, and concrete examples and implementations of its representative applications in bioinformatics. We start from the recent achievements of deep learning in the bioinformatics field, pointing out the problems which are suitable to use deep learning. After that, we introduce deep learning in an easy-to-understand fashion, from shallow neural networks to legendary convolutional neural networks, legendary recurrent neural networks, graph neural networks, generative adversarial networks, variational autoencoder, and the most recent state-of-the-art architectures. After that, we provide eight examples, covering five bioinformatics research directions and all the four kinds of data type, with the implementation written in Tensorflow and Keras. Finally, we discuss the common issues, such as overfitting and interpretability, that users will encounter when adopting deep learning methods and provide corresponding suggestions. The implementations are freely available at https://github.com/lykaust15/Deep_learning_examples.}
}

@article{khan18,
title = {A review on the application of deep learning in system health management},
journal = {Mechanical Systems and Signal Processing},
volume = {107},
pages = {241-265},
year = {2018},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2017.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0888327017306064},
author = {Samir Khan and Takehisa Yairi},
keywords = {Artificial intelligence, Deep learning, System health management, Real-time processing, Fault analysis, Maintenance},
abstract = {Given the advancements in modern technological capabilities, having an integrated health management and diagnostic strategy becomes an important part of a system’s operational life-cycle. This is because it can be used to detect anomalies, analyse failures and predict the future state based on up-to-date information. By utilising condition data and on-site feedback, data models can be trained using machine learning and statistical concepts. Once trained, the logic for data processing can be embedded on on-board controllers whilst enabling real-time health assessment and analysis. However, this integration inevitably faces several difficulties and challenges for the community; indicating the need for novel approaches to address this vexing issue. Deep learning has gained increasing attention due to its potential advantages with data classification and feature extraction problems. It is an evolving research area with diverse application domains and hence its use for system health management applications must been researched if it can be used to increase overall system resilience or potential cost benefits for maintenance, repair, and overhaul activities. This article presents a systematic review of artificial intelligence based system health management with an emphasis on recent trends of deep learning within the field. Various architectures and related theories are discussed to clarify its potential. Based on the reviewed work, deep learning demonstrates plausible benefits for fault diagnosis and prognostics. However, there are a number of limitations that hinder its widespread adoption and require further development. Attention is paid to overcoming these challenges, with future opportunities being enumerated.}
}

@inproceedings{niehues18,
  author    = {Jan Niehues and
               Ngoc{-}Quan Pham and
               Thanh{-}Le Ha and
               Matthias Sperber and
               Alex Waibel},
  editor    = {B. Yegnanarayana},
  title     = {Low-Latency Neural Speech Translation},
  booktitle = {Interspeech 2018, 19th Annual Conference of the International Speech
               Communication Association, Hyderabad, India, 2-6 September 2018},
  pages     = {1293--1297},
  publisher = {{ISCA}},
  year      = {2018},
  doi       = {10.21437/Interspeech.2018-1055},
}

@article{sezer20,
  title = {Financial time series forecasting with deep learning : A systematic literature review: 2005–2019},
  journal = {Applied Soft Computing},
  volume = {90},
  pages = {106181},
  year = {2020},
  issn = {1568-4946},
  doi = {https://doi.org/10.1016/j.asoc.2020.106181},
  url = {https://www.sciencedirect.com/science/article/pii/S1568494620301216},
  author = {Omer Berat Sezer and Mehmet Ugur Gudelek and Ahmet Murat Ozbayoglu},
  keywords = {Deep learning, Finance, Computational intelligence, Machine learning, Time series forecasting, CNN, LSTM, RNN},
  abstract = {Financial time series forecasting is undoubtedly the top choice of computational intelligence for finance researchers in both academia and the finance industry due to its broad implementation areas and substantial impact. Machine Learning (ML) researchers have created various models, and a vast number of studies have been published accordingly. As such, a significant number of surveys exist covering ML studies on financial time series forecasting. Lately, Deep Learning (DL) models have appeared within the field, with results that significantly outperform their traditional ML counterparts. Even though there is a growing interest in developing models for financial time series forecasting, there is a lack of review papers that solely focus on DL for finance. Hence, the motivation of this paper is to provide a comprehensive literature review of DL studies on financial time series forecasting implementation. We not only categorized the studies according to their intended forecasting implementation areas, such as index, forex, and commodity forecasting, but we also grouped them based on their DL model choices, such as Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), and Long-Short Term Memory (LSTM). We also tried to envision the future of the field by highlighting its possible setbacks and opportunities for the benefit of interested researchers.}
}

@INPROCEEDINGS{enshaei19,
  author={Enshaei, Nastaran and Naderkhani, Farnoosh},
  booktitle={2019 IEEE International Conference on Prognostics and Health Management (ICPHM)},
  title={Application of Deep Learning for Fault Diagnostic in Induction Machine’s Bearings},
  year={2019},
  volume={},
  number={},
  pages={1-7},
  doi={10.1109/ICPHM.2019.8819421}
}

@article{lou19,
  title = {An image-based deep learning framework for individualising radiotherapy dose: a retrospective analysis of outcome prediction},
  journal = {The Lancet Digital Health},
  volume = {1},
  number = {3},
  pages = {e136-e147},
  year = {2019},
  issn = {2589-7500},
  author = {Bin Lou and Semihcan Doken and Tingliang Zhuang and Danielle Wingerter and Mishka Gidwani and Nilesh Mistry and Lance Ladic and Ali Kamen and Mohamed E Abazeed},
}

@inproceedings{marin20,
  author    = {Marin Vlastelica Pogancic and
               Anselm Paulus and
               Vit Musil and
               Georg Martius and
               Michal Rolinek},
  title     = {Differentiation of Blackbox Combinatorial Solvers},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  year      = {2020},
}

@InProceedings{xu18,
  title = 	 {A Semantic Loss Function for Deep Learning with Symbolic Knowledge},
  author =       {Xu, Jingyi and Zhang, Zilu and Friedman, Tal and Liang, Yitao and Van den Broeck, Guy},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5502--5511},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/xu18h/xu18h.pdf},
  url = 	 {https://proceedings.mlr.press/v80/xu18h.html},
  abstract = 	 {This paper develops a novel methodology for using symbolic knowledge in deep learning. From first principles, we derive a semantic loss function that bridges between neural output vectors and logical constraints. This loss function captures how close the neural network is to satisfying the constraints on its output. An experimental evaluation shows that it effectively guides the learner to achieve (near-)state-of-the-art results on semi-supervised multi-class classification. Moreover, it significantly increases the ability of the neural network to predict structured objects, such as rankings and paths. These discrete concepts are tremendously difficult to learn, and benefit from a tight integration of deep learning and symbolic reasoning methods.}
}

@INPROCEEDINGS{wong12,
  author={Wong, Lawson L.S. and Kaelbling, Leslie Pack and Lozano-Pérez, Tomás},
  booktitle={2012 IEEE International Conference on Robotics and Automation},
  title={Collision-free state estimation},
  year={2012},
  volume={},
  number={},
  pages={223-228},
  doi={10.1109/ICRA.2012.6225309}
}

@ARTICLE{lu13,
  author={Lu, Wei-Lwun and Ting, Jo-Anne and Little, James J. and Murphy, Kevin P.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title={Learning to Track and Identify Players from Broadcast Sports Videos},
  year={2013},
  volume={35},
  number={7},
  pages={1704-1716},
  doi={10.1109/TPAMI.2012.242}
}

@book{darwiche09,
  title={Modeling and Reasoning with Bayesian Networks},
  DOI={10.1017/CBO9780511811357},
  publisher={Cambridge University Press},
  author={Darwiche, Adnan},
  year={2009}
}

@article{libra,
    title        =  {The Libra Toolkit for Probabilistic Models},
    author    =  {Lowd, Daniel and Rooshenas, Amirmohammad},
    journal   =  {Journal of Machine Learning Research},
    year       =  {2015},
    volume  =  {16},
    pages     =  {2459-2463}
}

@inproceedings{rasmussen00,
 author = {Rasmussen, Carl},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 pages = {},
 publisher = {MIT Press},
 title = {The Infinite Gaussian Mixture Model},
 url = {https://proceedings.neurips.cc/paper/1999/file/97d98119037c5b8a9663cb21fb8ebf47-Paper.pdf},
 volume = {12},
 year = {2000}
}

@article{xolani20,
title = {Statistical and machine learning models in credit scoring: A systematic literature survey},
journal = {Applied Soft Computing},
volume = {91},
pages = {106263},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106263},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620302039},
author = {Xolani Dastile and Turgay Celik and Moshe Potsane},
keywords = {Credit scoring, Statistical learning, Machine learning, Deep learning, Systematic literature survey},
abstract = {In practice, as a well-known statistical method, the logistic regression model is used to evaluate the credit-worthiness of borrowers due to its simplicity and transparency in predictions. However, in literature, sophisticated machine learning models can be found that can replace the logistic regression model. Despite the advances and applications of machine learning models in credit scoring, there are still two major issues: the incapability of some of the machine learning models to explain predictions; and the issue of imbalanced datasets. As such, there is a need for a thorough survey of recent literature in credit scoring. This article employs a systematic literature survey approach to systematically review statistical and machine learning models in credit scoring, to identify limitations in literature, to propose a guiding machine learning framework, and to point to emerging directions. This literature survey is based on 74 primary studies, such as journal and conference articles, that were published between 2010 and 2018. According to the meta-analysis of this literature survey, we found that in general, an ensemble of classifiers performs better than single classifiers. Although deep learning models have not been applied extensively in credit scoring literature, they show promising results.}
}
