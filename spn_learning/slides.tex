%&tex

\documentclass[usenames,dvipsnames]{beamer}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{thmtools,thm-restate}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage[singlelinecheck=false]{caption}
\usepackage[backend=biber,url=true,doi=true,eprint=false,style=authoryear]{biblatex}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{xcolor}

\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{fit}
\usetikzlibrary{calc}

\usepackage{tkz-graph}

\addbibresource{references.bib}
\usetheme{metropolis}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\Val}{\text{Val}}
\DeclareMathOperator*{\Ch}{\text{Ch}}
\DeclareMathOperator*{\Pa}{\text{Pa}}
\DeclareMathOperator*{\Sc}{\text{Sc}}
\newcommand{\ov}{\overline}
\newcommand{\tsup}{\textsuperscript}

\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}

\newcommand{\algorithmautorefname}{Algorithm}
\algrenewcommand\algorithmicrequire{\textbf{Input}}
\algrenewcommand\algorithmicensure{\textbf{Output}}
\algnewcommand{\LineComment}[1]{\State\,\(\triangleright\) #1}

\newcommand{\Left}{\text{LEFT}}
\newcommand{\Right}{\text{RIGHT}}
\newcommand{\Up}{\text{UP}}

\newcommand{\set}[1]{\mathbf{#1}}
\newcommand{\pr}{\text{P}}
\newcommand{\eps}{\varepsilon}
\newcommand{\ddspn}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\iddspn}[2]{\partial#1/\partial#2}
\newcommand{\indep}{\perp}
\renewcommand{\implies}{\Rightarrow}

\newcommand{\bigo}{\mathcal{O}}
\newcommand{\mbf}[1]{\mathbf{#1}}

\setbeamertemplate{theorems}[ams style]

\setbeamersize{description width=1.0cm}

\lstset{frameround=fttt,
	numbers=left,
	breaklines=true,
	keywordstyle=\bfseries,
	basicstyle=\ttfamily,
}

\newcommand{\code}[1]{\lstinline[mathescape=true]{#1}}
\newcommand{\mcode}[1]{\lstinline[mathescape]!#1!}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\title{Learning Sum-Product Networks}
\date{}
\author{Renato Lui Geh}
\institute{Institute of Mathematics and Statistics --- University of SÃ£o Paulo}

\begin{document}

\maketitle

\section{Introduction}

\begin{frame}
  \frametitle{Definition}
  \begin{definition}[Generalized sum-product network]~\\
    A sum-product network (SPN) is a DAG where each node $n$ is either:
    \begin{enumerate}
      \item A tractable univariate probability distribution;
      \item A product of SPNs: $v_n=\prod_{j\in\Ch(n)}v_j$; or
      \item A weighted sum of SPNs: $v_n=\sum_{j\in\Ch(n)}w_{n,j}v_j$.
    \end{enumerate}
    Where $v_n$ is the value of node $n$, $\Ch(n)$ its set of children and $w_{n,j}$ the weight of
    edge $n\to j$.
  \end{definition}
\end{frame}

\begin{frame}
  \frametitle{Scope}

  The scope $\Sc(n)$ of node $n$ is the union of the scope of its children. The scope of a leaf is
  the set of all variables in the distribution. Let $S$ be the root of the SPN below:
  \vspace{-0.75cm}

  \begin{center}
    \begin{tikzpicture}
      \begin{scope}[every node/.style={circle,draw,inner sep=1pt}]
        \node[label={[label distance=-1.0cm]above:{\scriptsize$\Sc(S)=\{X_1,X_2\}$}}] (root) at (0, 0) {+};
        \node[label=left:{\scriptsize$\{X_1,X_2\}$}] (p1) at (-1.5, -1.25) {$\times$};
        \node[label=right:{\scriptsize$\{X_1,X_2\}$}] (p2) at (0, -1.25) {$\times$};
        \node[label=right:{\scriptsize$\{X_1,X_2\}$}] (p3) at (1.75, -1.25) {$\times$};
        \node[label=left:\scriptsize$\{X_1\}$] (s1) at (-2.5, -2.5) {+};
        \node[label=right:\scriptsize$\{X_1\}$] (s2) at (-1.0, -2.5) {+};
        \node[label=left:\scriptsize$\{X_2\}$] (s3) at (1.0, -2.5) {+};
        \node[label=right:\scriptsize$\{X_2\}$] (s4) at (2.5, -2.5) {+};
        \node[draw=none,fill=none,rectangle] (x11) at (-2.5, -3.75) {$\mathcal{X}_1^1$};
        \node[draw=none,fill=none,rectangle] (x12) at (-0.75, -3.75) {$\mathcal{X}_1^2$};
        \node[draw=none,fill=none,rectangle] (x21) at (0.75, -3.75) {$\mathcal{X}_2^1$};
        \node[draw=none,fill=none,rectangle] (x22) at (2.5, -3.75) {$\mathcal{X}_2^2$};
      \end{scope}

      \begin{scope}[every path/.style={->}]
        \draw (root) -- node[left]{\tiny$0.5$} (p1);
        \draw (root) -- node{\tiny$0.2$} (p2);
        \draw (root) -- node[right]{\tiny$0.3$} (p3);
        \draw (p1) -- (s1);
        \draw (p1) -- (s3);
        \draw (p2) -- (s2);
        \draw (p2) -- (s3);
        \draw (p3) -- (s2);
        \draw (p3) -- (s4);
        \draw (s1) -- node[left]{\tiny$0.3$} (x11);
        \draw (s1) -- node[left]{\tiny$0.7$} (x12);
        \draw (s2) -- node[right]{\tiny$0.2$} (x11);
        \draw (s2) -- node[right]{\tiny$0.8$} (x12);
        \draw (s3) -- node[left]{\tiny$0.6$} (x21);
        \draw (s3) -- node[left]{\tiny$0.4$} (x22);
        \draw (s4) -- node[right]{\tiny$0.9$} (x21);
        \draw (s4) -- node[right]{\tiny$0.1$} (x22);
      \end{scope}
    \end{tikzpicture}
  \end{center}

\end{frame}

\begin{frame}
  \frametitle{Validity}

  \begin{definition}[Validity]~\\
    Let $S$ be an SPN. If $S$ correctly computes and marginalizes an unnormalized probability
    $\phi(\mbf{X})$, then it is said to be \emph{valid}.
  \end{definition}~\\

  If for every sum node $n$

  \begin{equation*}
    \forall j\in\Ch(n), w_{n,j} \geq 0\text{ and }\sum_{j\in\Ch(n)}w_{n,j} = 1
  \end{equation*}

  then $S$ represents the probability distribution itself.\\~\\

  A \textbf{sufficient}, yet not necessary, condition for validity is \emph{completeness} and
  \emph{consistency} (\cite{poon-domingos}).

\end{frame}

\begin{frame}
  \frametitle{Completeness}
  \begin{definition}[Completeness]~\\
    An SPN $S$ is said to be complete, iff for each sum node $s\in S$, all children of $s$ have
    same scope.
  \end{definition}
  \vspace{-0.5cm}

  \begin{center}
    \begin{tikzpicture}
      \begin{scope}[every node/.style={circle,draw,inner sep=1pt}]
        \node[color=blue,thick,label={[label
          distance=-1.0cm]above:{\scriptsize$\Sc(S)=\{X_1,X_2\}$}}] (root) at (0, 0) {\textbf{+}};
        \node[label=left:{\scriptsize$\{X_1,X_2\}$}] (p1) at (-1.5, -1.25) {$\times$};
        \node[label=right:{\scriptsize$\{X_1,X_2\}$}] (p2) at (0, -1.25) {$\times$};
        \node[label=right:{\scriptsize$\{X_1,X_2\}$}] (p3) at (1.75, -1.25) {$\times$};
        \node[color=blue,thick,label=left:\scriptsize$\{X_1\}$] (s1) at (-2.5, -2.5) {\textbf{+}};
        \node[color=blue,thick,label=right:\scriptsize$\{X_1\}$] (s2) at (-1.0, -2.5) {\textbf{+}};
        \node[color=blue,thick,label=left:\scriptsize$\{X_2\}$] (s3) at (1.0, -2.5) {\textbf{+}};
        \node[color=blue,thick,label=right:\scriptsize$\{X_2\}$] (s4) at (2.5, -2.5) {\textbf{+}};
        \node[draw=none,fill=none,rectangle] (x11) at (-2.5, -3.75) {$\mathcal{X}_1^1$};
        \node[draw=none,fill=none,rectangle] (x12) at (-0.75, -3.75) {$\mathcal{X}_1^2$};
        \node[draw=none,fill=none,rectangle] (x21) at (0.75, -3.75) {$\mathcal{X}_2^1$};
        \node[draw=none,fill=none,rectangle] (x22) at (2.5, -3.75) {$\mathcal{X}_2^2$};
      \end{scope}

      \begin{scope}[every path/.style={->}]
        \draw (root) -- node[left]{\tiny$0.5$} (p1);
        \draw (root) -- node{\tiny$0.2$} (p2);
        \draw (root) -- node[right]{\tiny$0.3$} (p3);
        \draw (p1) -- (s1);
        \draw (p1) -- (s3);
        \draw (p2) -- (s2);
        \draw (p2) -- (s3);
        \draw (p3) -- (s2);
        \draw (p3) -- (s4);
        \draw (s1) -- node[left]{\tiny$0.3$} (x11);
        \draw (s1) -- node[left]{\tiny$0.7$} (x12);
        \draw (s2) -- node[right]{\tiny$0.2$} (x11);
        \draw (s2) -- node[right]{\tiny$0.8$} (x12);
        \draw (s3) -- node[left]{\tiny$0.6$} (x21);
        \draw (s3) -- node[left]{\tiny$0.4$} (x22);
        \draw (s4) -- node[right]{\tiny$0.9$} (x21);
        \draw (s4) -- node[right]{\tiny$0.1$} (x22);
      \end{scope}
    \end{tikzpicture}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Consistency}
  \begin{definition}[Consistency]~\\
    An SPN $S$ is said to be consistent, iff no variable appears with a value $v$ in one child of a
    product node, and valued $u$, with $u\neq v$, in another.
  \end{definition}

  \begin{center}
    \begin{tikzpicture}
      \begin{scope}[every node/.style={circle,draw,inner sep=1pt}]
        \node (root) at (0, 0) {+};
        \node[color=black!30!green,thick] (p1) at (-1.5, -1.25) {$\boldsymbol{\times}$};
        \node[color=red,thick] (p2) at (0, -1.25) {$\boldsymbol{\times}$};
        \node[color=black!30!green,thick] (p3) at (1.75, -1.25) {$\boldsymbol{\times}$};
        \node (s1) at (-2.5, -2.5) {+};
        \node[color=red,thick] (s2) at (-0.75, -2.5) {+};
        \node[color=red,thick] (s3) at (0.75, -2.5) {+};
        \node (s4) at (2.5, -2.5) {+};
        \node[label={[label
          distance=0.1cm,color=blue]below:{\scriptsize$0$}},draw=none,fill=none,rectangle] (x11) at
          (-2.5, -3.75) {\scriptsize$[X_2=0]$};
        \node[color=red,thick,label={[label
          distance=0.1cm,color=red]below:{\scriptsize$1$}},draw=none,fill=none,rectangle] (x12) at
          (-0.75, -3.75) {\scriptsize$[X_1=0]$};
        \node[color=red,thick,label={[label
          distance=0.1cm,color=red]below:{\scriptsize$0$}},draw=none,fill=none,rectangle] (x21) at
          (0.75, -3.75) {\scriptsize$[X_1=1]$};
        \node[label={[label
          distance=0.1cm,color=blue]below:{\scriptsize$1$}},draw=none,fill=none,rectangle] (x22) at
          (2.5, -3.75) {\scriptsize$[X_2=0]$};
      \end{scope}

      \begin{scope}[every path/.style={->}]
        \draw (root) -- node[left]{\tiny$0.5$} (p1);
        \draw (root) -- node{\tiny$0.2$} (p2);
        \draw (root) -- node[right]{\tiny$0.3$} (p3);
        \draw (p1) -- (s1);
        \draw (p1) -- (s2);
        \draw[thick,color=red] (p2) -- (s2);
        \draw[thick,color=red] (p2) -- (s3);
        \draw (p3) -- (s3);
        \draw (p3) -- (s4);
        \draw (s1) -- node[left]{\tiny$0.3$} (x11);
        \draw (s1) -- node[left]{\tiny$0.7$} (x12);
        \draw (s2) -- node[right]{\tiny$0.2$} (x11);
        \draw[thick,color=red] (s2) -- node[right]{\tiny$0.8$} (x12);
        \draw[thick,color=red] (s3) -- node[left]{\tiny$0.6$} (x21);
        \draw (s3) -- node[left]{\tiny$0.4$} (x22);
        \draw (s4) -- node[right]{\tiny$0.9$} (x21);
        \draw (s4) -- node[right]{\tiny$0.1$} (x22);
      \end{scope}
    \end{tikzpicture}
  \end{center}\vspace{-1.0cm}

  \begin{equation*}
    X=\{X_1=0,X_2=1\}
  \end{equation*}

\end{frame}

\begin{frame}
  \frametitle{Decomposability}

  \begin{definition}[Decomposability]~\\
    An SPN is decomposable iff no variable appears in more than one child of a product node (i.e.
    scopes are disjoint).
  \end{definition}\vspace{-0.5cm}

  \begin{center}
    \begin{tikzpicture}
      \begin{scope}[every node/.style={circle,draw,inner sep=1pt}]
        \node[label={[label distance=-1.0cm]above:{\scriptsize$\Sc(S)=\{X_1,X_2\}$}}] (root) at (0, 0) {+};
        \node[color=blue,thick,label=left:{\scriptsize$\{X_1,X_2\}$}] (p1) at (-1.5, -1.25)
          {$\boldsymbol{\times}$};
        \node[color=blue,thick,label=right:{\scriptsize$\{X_1,X_2\}$}] (p2) at (0, -1.25)
          {$\boldsymbol{\times}$};
        \node[color=blue,thick,label=right:{\scriptsize$\{X_1,X_2\}$}] (p3) at (1.75, -1.25)
          {$\boldsymbol\times$};
        \node[label=left:\scriptsize$\{X_1\}$] (s1) at (-2.5, -2.5) {+};
        \node[label=right:\scriptsize$\{X_1\}$] (s2) at (-1.0, -2.5) {+};
        \node[label=left:\scriptsize$\{X_2\}$] (s3) at (1.0, -2.5) {+};
        \node[label=right:\scriptsize$\{X_2\}$] (s4) at (2.5, -2.5) {+};
        \node[draw=none,fill=none,rectangle] (x11) at (-2.5, -3.75) {$\mathcal{X}_1^1$};
        \node[draw=none,fill=none,rectangle] (x12) at (-0.75, -3.75) {$\mathcal{X}_1^2$};
        \node[draw=none,fill=none,rectangle] (x21) at (0.75, -3.75) {$\mathcal{X}_2^1$};
        \node[draw=none,fill=none,rectangle] (x22) at (2.5, -3.75) {$\mathcal{X}_2^2$};
      \end{scope}

      \begin{scope}[every path/.style={->}]
        \draw (root) -- node[left]{\tiny$0.5$} (p1);
        \draw (root) -- node{\tiny$0.2$} (p2);
        \draw (root) -- node[right]{\tiny$0.3$} (p3);
        \draw (p1) -- (s1);
        \draw (p1) -- (s3);
        \draw (p2) -- (s2);
        \draw (p2) -- (s3);
        \draw (p3) -- (s2);
        \draw (p3) -- (s4);
        \draw (s1) -- node[left]{\tiny$0.3$} (x11);
        \draw (s1) -- node[left]{\tiny$0.7$} (x12);
        \draw (s2) -- node[right]{\tiny$0.2$} (x11);
        \draw (s2) -- node[right]{\tiny$0.8$} (x12);
        \draw (s3) -- node[left]{\tiny$0.6$} (x21);
        \draw (s3) -- node[left]{\tiny$0.4$} (x22);
        \draw (s4) -- node[right]{\tiny$0.9$} (x21);
        \draw (s4) -- node[right]{\tiny$0.1$} (x22);
      \end{scope}
    \end{tikzpicture}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Decomposability vs Consistency}

  \textbf{Decomposability} implies \textbf{consistency}.
  \vfill

  But \textbf{decomposability} is much easier for learning, and allows for an interpretation of
  product nodes as \emph{independencies} between variables.
  \vfill

  \cite{theoretical-spn} shows \textbf{decomposable} SPNs are as representable as solely
  \textbf{consistent} ones.
\end{frame}


\section{Learning}

\begin{frame}
  \frametitle{Learning types}

  Two main types of learning:

  \begin{description}
    \item[Structure learning:]~\\
      Learn graph structure from data.
    \item[Parameter learning:]~\\
      Learn weights from data given a fixed graph structure.
  \end{description}

  Both usually attempt to optimize log-likelihood.

\end{frame}

\begin{frame}
  \frametitle{Structure learning}

  \begin{itemize}
    \item Structure learning:
      \begin{enumerate}
        \item \textbf{Poon-Domingos dense architecture}~(\cite{poon-domingos});
        \item \textbf{Gens-Domingos LearnSPN}~(\cite{gens-domingos});
        \item \textbf{Dennis-Ventura clustering architecture}~(\cite{clustering});
        \item \textbf{Random Tensorized SPNs (RAT-SPNs)}~(\cite{deep-learn-spn});
        \item Indirect-Direct SPNs (ID-SPNs)~(\cite{id-spn});
        \item LearnSPN+Chow-Liu Trees (LearnSPN-BTB)~(\cite{vergari-mauro}).
      \end{enumerate}
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Parameter learning}

  \begin{itemize}
    \item Parameter learning:
      \begin{enumerate}
        \item \textbf{Generative Gradient Descent}~(\cite{poon-domingos,diff-approach-darwiche});
        \item \textbf{Discriminative Gradient Descent}~(\cite{discriminative});
        \item Expectation-Maximization~(\cite{poon-domingos});
        \item Extended Baum-Welch~(\cite{baum-welch});
        \item Collapsed Variational Inference~(\cite{variational-spn});
        \item Bayesian Moment Matching~(\cite{bayesian-moment}).
      \end{enumerate}
  \end{itemize}
\end{frame}

\section{Parameter learning}

\begin{frame}
  \frametitle{Generative vs Discriminative Gradient Descent}

  \begin{description}
    \item[Generative:]~\\
      \begin{itemize}
        \item Optimize log-likelihood of $P(X,Y)$
        \item Gradient: $\ddspn{}{W}\log P(X,Y)$
        \item E.g. completion
      \end{itemize}
    \item[Discriminative:]~\\
      \begin{itemize}
        \item Optimize log-likelihood of $P(Y|X)$
        \item Gradient: $\ddspn{}{W}\log P(Y|X)$
        \item E.g. classification
      \end{itemize}
  \end{description}

  \centering{\textbf{Generative} representation is able to extract its \textbf{discriminative}.}

  \begin{equation*}
    P(Y|X)=\frac{P(X,Y)}{P(Y)}
  \end{equation*}
\end{frame}

\begin{frame}
  \frametitle{Derivatives I}

  Let $S$ be an SPN, and $W$ the set of weights of $S$. Denote by $S_n$ the sub-SPN rooted at node
  $n$.

  \centering{\textbf{Objective:} find gradient $\ddspn{}{W}\log S$.}

  That is, compute each component $\iddspn{S}{w_{n,j}}$, for each edge $n\to j$.

  \begin{align*}
    \ddspn{S}{w_{n,j}}(X)&=\ddspn{S}{S_n}\ddspn{S_n}{w_{n,j}}(X)\\
      &=\ddspn{S}{S_n}\ddspn{}{w_{n,j}}\left(\sum_{i\in\Ch(n)}w_{n,i}S_i(X)\right)\\
      &=\ddspn{S}{S_n}S_j(X).
  \end{align*}

  We now need to find a form for derivative $\ddspn{S}{S_n}$.
\end{frame}

\begin{frame}
  \frametitle{Derivatives II}

  Let's find the sub-SPN derivative $\ddspn{S}{S_j}$. From chain rule:

  \begin{align*}
    \ddspn{S}{S_j}(X)&=\sum_{n\in\Pa(j)}\ddspn{S}{S_n}\ddspn{S_n}{S_j}(X)\\
      &=\underbrace{\sum_{\substack{n\in\Pa(j)\\n:\text{
        sum}}}\ddspn{S}{S_n}\ddspn{S_n}{S_j}(X)}_{(\ast)}+
      \underbrace{\sum_{\substack{n\in\Pa(j)\\n:\text{
        product}}}\ddspn{S}{S_n}\ddspn{S_n}{S_j}(X)}_{(\ast\ast)}
  \end{align*}

  Let's analyze two cases: when $n$ is a sum node $(\ast)$, and when it's a product node
  $(\ast\ast)$.
\end{frame}

\begin{frame}
  \frametitle{Derivatives III}

  \textbf{Case 1:} when $n$ is a sum node.

  \begin{align*}
    (\ast)&=\sum_{\substack{n\in\Pa(j)\\n:\text{ sum}}}\ddspn{S}{S_n}\ddspn{S_n}{S_j}(X)\\
          &=\sum_{\substack{n\in\Pa(j)\\n:\text{ sum}}}\ddspn{S}{S_n}\ddspn{}{S_j}\left(\sum_{i\in\Ch(n)}
    w_{n,i}S_i(X)\right)\\
          &=\sum_{\substack{n\in\Pa(j)\\n:\text{ sum}}}\ddspn{S}{S_n}w_{n,j}
  \end{align*}
\end{frame}

\begin{frame}
  \frametitle{Derivatives IV}

  \textbf{Case 2:} when $n$ is a product node.

  \begin{align*}
    (\ast\ast)&=\sum_{\substack{n\in\Pa(j)\\n:\text{ product}}}\ddspn{S}{S_n}\ddspn{S_n}{S_j}(X)\\
      &=\sum_{\substack{n\in\Pa(j)\\n:\text{ product}}}\ddspn{S}{S_n}\ddspn{}{S_j}\left(\prod_{i\in\Ch(n)}
      S_i(X)\right)\\
      &=\sum_{\substack{n\in\Pa(j)\\n:\text{ product}}}\ddspn{S}{S_n}\prod_{k\in
      \Ch(n)\setminus\{j\}}S_k
  \end{align*}
\end{frame}

\begin{frame}
  \frametitle{Derivatives V}

  Going back to our original formula and using the derived forms of $(\ast)$ and $(\ast\ast)$:

  \begin{align*}
    \ddspn{S}{S_j}(X)&=\sum_{\substack{n\in\Pa(j)\\n:\text{
          sum}}}\ddspn{S}{S_n}\ddspn{S_n}{S_j}(X)&+&\sum_{\substack{n\in\Pa(j)\\n:\text{
      product}}}\ddspn{S}{S_n}\ddspn{S_n}{S_j}(X)\\
                     &=\sum_{\substack{n\in\Pa(j)\\n:\text{
          sum}}}\ddspn{S}{S_n}w_{n,j}&+&\sum_{\substack{n\in\Pa(j)\\n:\text{
                       product}}}\ddspn{S}{S_n}\prod_{k\in \Ch(n)\setminus\{j\}}S_k
  \end{align*}

  \begin{center}
    \textbf{Remark:} when $j$ is the root node: $\ddspn{S}{S_j}=\ddspn{S}{S}=1$.
  \end{center}

  The above form lends itself nicely to an algorithmic format.
\end{frame}

\begin{frame}
  \frametitle{Derivatives VI}

  \begin{algorithm}[H]
    \caption{\code{Backprop}: Backpropagation derivation on SPNs}
    \begin{algorithmic}[1]
      \Require A valid SPN $S$ with pre-computed probabilities $S_n(X)$
      \Ensure Partial derivatives of $S$ with respect to every node and weight
      \State Initialize $\ddspn{S}{S_n}=0$ except $\ddspn{S}{S}=1$
      \For{each node $n\in S$ in top-down order}
        \If{$n$ is sum node}
          \For{all $j\in\Ch(n)$}
            \State $\ddspn{S}{S_j}\gets\ddspn{S}{S_j}+w_{n,j}\ddspn{S}{S_n}$
            \State $\ddspn{S}{w_{n,j}}\gets\ddspn{S}{S_n}S_j$
          \EndFor%
        \Else%
          \For{all $j\in\Ch(n)$}
            \State $\ddspn{S}{S_j}\gets\ddspn{S}{S_j}+\ddspn{S}{S_n}\prod_{k\in\Ch(n)\setminus
              \{j\}}S_k$
          \EndFor%
        \EndIf
      \EndFor%
    \end{algorithmic}
  \end{algorithm}
\end{frame}

\begin{frame}
  \frametitle{Generative gradient descent}

  Let's go back to our original objective of finding the gradient. For the \textbf{generative}
  (joint distribution) case:

  \begin{align*}
    \ddspn{}{W}\log P(X,Y)&=\ddspn{}{W}\log S(X,Y)\\
                          &=\frac{1}{S(X,Y)}\ddspn{S}{W}(X,Y)\propto\ddspn{S}{W}(X,Y)
  \end{align*}

  So it is sufficient to find $\ddspn{S}{W}$, which we already have. Our weight update is then:

  \begin{equation*}
    \Delta w_{n,j}=\eta\ddspn{S}{w_{n,j}}(X,Y)
  \end{equation*}
\end{frame}

\begin{frame}
  \frametitle{Discriminative gradient descent}

  For the \textbf{discriminative} (conditional distribution) case:

  \begin{align*}
    \ddspn{}{W}\log P(Y|X)&=\ddspn{}{W}\log\left(\frac{P(Y,X)}{P(X)}\right)\\
                          &=\ddspn{}{W}\log P(Y,X)&-\quad&\ddspn{}{W}\log P(X)\\
                          &=\frac{1}{S(Y,X)}\ddspn{}{W}S(Y,X)&-\quad&\frac{1}{S(X)}\ddspn{}{W}S(X)
  \end{align*}

  Weight updates will take the following form:

  \begin{equation*}
    \Delta w_{n,j}=\eta\left(\frac{1}{S(Y,X)}\ddspn{S(Y,X)}{w_{n,j}}-\frac{1}{S(X)}
      \ddspn{S(X)}{w_{n,j}}\right)
  \end{equation*}

\end{frame}

\begin{frame}
  \frametitle{Soft vs hard derivation}

  Results derived in the previous slides are called \textbf{soft}. Soft gradient means weight
  updates are derivatives of network evaluations. The \textbf{deeper} the network, the
  \textbf{fainter} the signal.

  \begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
      \centering\includegraphics[width=\textwidth]{imgs/softgrad.png}
      \captionsetup{justification=centering}
      \caption{Soft gradient}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\linewidth}
      \centering\includegraphics[width=\textwidth]{imgs/hardgrad.png}
      \captionsetup{justification=centering}
      \caption{Hard gradient}
    \end{subfigure}
  \end{figure}

  This is called \textbf{gradient diffusion}. A solution to this is \textbf{hard} gradient descent.
\end{frame}

\begin{frame}
  \frametitle{Hard derivatives I}

  Let $S$ be an SPN. Instead of $\ddspn{S}{W}$, we'll derive $\ddspn{M}{W}$, where $M$ is the
  Max-Product Network (MPN) of $S$. $M$ can be extracted from $S$ by replacing sums with max nodes.

  \begin{description}
    \item[Soft:]~\\
      \begin{itemize}
        \item $\iddspn{S}{W}$
        \item $W$ is the set of weights of $S$
        \item Messages are derivatives
      \end{itemize}
    \item[Hard:]~\\
      \begin{itemize}
        \item $\iddspn{M}{W}$
        \item $W$ is the multiset of weights that a forward pass through $M$ visits
        \item Messages are counts
      \end{itemize}
  \end{description}
\end{frame}

\begin{frame}
  \frametitle{Hard derivatives II}

  \begin{center}
    \textbf{Objective:} find gradient $\ddspn{M}{W}$
  \end{center}

  We know that $M(X)=\prod_{w_i\in W} w_i^{c_i}$, where $c_i$ is the number of times $w_i$ appears
  in $W$. Let's take the logarithm of $M$ on each component:

  \begin{align*}
    \ddspn{\log M}{w_{n,j}}(X)&=\ddspn{}{w_{n,j}}\log\left(\prod_{w_i\in W}w_i^{c_i}\right)\\
                              &=\frac{1}{\prod_{w_i\in W}w_i^{c_i}}\cdot c_{n,j}w_{n,j}^{c_{n,j}-1}
    \cdot\prod_{w_i\in W\setminus\{w_{n,j}\}}w_i^{c_i}\\
                              &=c_{n,j}\frac{w_{n,j}^{c_{n,j}-1}}{w_{n,j}^{c_{n,j}}}
                              =\frac{c_{n,j}}{w_{n,j}}
  \end{align*}
\end{frame}

\begin{frame}
  \frametitle{Hard generative gradient descent}

  From our previous result, we know that:

  \begin{equation*}
    \ddspn{\log M}{w_{n,j}}(X)=\frac{c_{n,j}}{w_{n,j}}
  \end{equation*}

  But that's exactly the log-likelihood for the generative case! This gives us the following weight
  update:

  \begin{equation*}
    \Delta w_{n,j}=\eta\frac{c_{n,j}}{w_{n,j}}
  \end{equation*}

  Note how $c_{n,j}$ is an integer, and $w_{n,j}\in [0,1]$, meaning the signal passed at learning
  does not depend on network size or depth, avoiding the problem of gradient diffusion.
\end{frame}

\begin{frame}
  \frametitle{Hard discriminative gradient descent I}

  For the discriminative case we want:
  \begin{equation*}
    \ddspn{}{W}\log\tilde{P}(Y|X)=\ddspn{}{W}\log\left(\frac{\tilde{P}(Y,X)}{\tilde{P}(X)}\right)=
    \ddspn{}{W}\log\left(\frac{M(Y,X)}{M(X)}\right)
  \end{equation*}

  Where $\tilde{P}$ is the MAP. Apply chain rule:
  \begin{align*}
    \ddspn{}{W}\log\left(\frac{M(Y,X)}{M(X)}\right)&=\ddspn{}{W}\log M(Y,X)-\ddspn{}{W}\log M(X)
  \end{align*}

  For each component:
  \begin{align*}
    \ddspn{}{w_{n,j}}\log\tilde{P}(Y|X)&=\ddspn{}{w_{n,j}}\log M(Y,X)- \ddspn{}{w_{n,j}}\log M(X)\\
                                       &=\ddspn{}{w_{n,j}}c_{n,j}-\ddspn{}{w_{n,j}}\hat{c}_{n,j}\\
                                       &=\frac{\Delta c_{n,j}}{w_{n,j}}
  \end{align*}
\end{frame}

\begin{frame}
  \frametitle{Hard discriminative gradient descent II}

  Visually, $\Delta c_{n,j}$ is the difference between the path of evaluation $M(Y, X)$ and $M(X)$.

  \begin{figure}[h]
    \centering
    \begin{minipage}{0.3\textwidth}
      \includegraphics[width=\linewidth]{imgs/hard_diff_0.png}
      \captionsetup{justification=centering}
      \caption*{$\ddspn{}{W}\log M(Y,X)$}
    \end{minipage}
    $-$
    \begin{minipage}{0.3\textwidth}
      \includegraphics[width=\linewidth]{imgs/hard_diff_1.png}
      \captionsetup{justification=centering}
      \caption*{$\ddspn{}{W}\log M(X)$}
    \end{minipage}
    $=$
    \begin{minipage}{0.3\textwidth}
      \includegraphics[width=\linewidth]{imgs/hard_diff_2.png}
      \captionsetup{justification=centering}
      \caption*{$\nabla\log\tilde{P}(Y|X)$}
    \end{minipage}
  \end{figure}

  Edges in blue represent positive values, red are negative values and uncolored edges have zero
  value.
\end{frame}

\begin{frame}
  \frametitle{Summary}

  \scriptsize
  \begin{center}\footnotesize\textbf{Generative Gradient Descent}\end{center}\vspace{-0.25cm}
  \begin{table}[h]
    \centering
    \begin{tabular}{l|l}
      \hline
      \multicolumn{1}{c}{\bfseries Inference} & \multicolumn{1}{c}{\bfseries Weight updates}\\
      \hline & \\
      \textbf{Soft} & \(\displaystyle \Delta w_{n,j}=\eta\ddspn{S}{w_{n,j}}(X, Y)\) \\
      & \\
      \textbf{Hard} & \(\displaystyle \Delta w_{n,j}=\eta\frac{c_{n,j}}{w_{n,j}}\) \\
      & \\
      \hline
    \end{tabular}
  \end{table}

  \begin{center}\footnotesize\textbf{Discriminative Gradient Descent}\end{center}\vspace{-0.25cm}
  \begin{table}[h]
    \centering
    \begin{tabular}{l|l}
      \hline
      \multicolumn{1}{c}{\bfseries Inference} & \multicolumn{1}{c}{\bfseries Weight updates}\\
      \hline & \\
      \textbf{Soft} & \(\displaystyle \Delta
        w_{n,j}=\eta\left(\frac{1}{S(Y,X)}\ddspn{S(Y,X)}{w_{n,j}}-\frac{1}{S(X)}
          \ddspn{S(X)}{w_{n,j}}\right)\) \\
      & \\
      \textbf{Hard} & \(\displaystyle \Delta w_{n,j}=\eta\frac{\Delta c_{n,j}}{w_{n,j}}\) \\
      & \\
      \hline
    \end{tabular}
  \end{table}

\end{frame}

\section{Structure learning}

\begin{frame}
  \frametitle{Preliminaries I}

  \begin{definition}[Dataset]~\\
    A dataset is a matrix $D\in M_{m\times n}(\mathbb{Z})$, where {\color{blue}\textbf{rows}} are
    {\color{blue}\textbf{instances}} and {\color{red}\textbf{columns}} represent
    {\color{red}\textbf{variables}}.
  \end{definition}

  \begin{example}~\\
    Let $D$ be a $3\times 2$ black and white image dataset, where each pixel can take a value 0
    (black) or 1 (white).

    \begin{table}
      \begin{tabular}{c|cccccc}
        $D$ & \color{red}$X_1$ & \color{red}$X_2$ & \color{red}$X_3$ & \color{red}$X_4$ &
        \color{red}$X_5$ & \color{red}$X_6$\\
        \hline
        \color{blue}$I_1$ & 1 & 0 & 1 & 1 & 0 & 1\\
        \color{blue}$I_2$ & 0 & 1 & 0 & 0 & 1 & 0\\
      \end{tabular}
    \end{table}\vspace{-0.5cm}
    \begin{align*}
      \mbf{X}&=\{X_1,\ldots,X_n\}\quad&\text{is}&\text{ the set of random {\color{red}variables}
      (e.g.  pixels) of $D$.}\\
      \mbf{I}&=\{I_1,\ldots,I_m\}\quad&\text{is}&\text{ the set of {\color{blue}instances} of $D$.}
    \end{align*}
  \end{example}

\end{frame}

\begin{frame}
  \frametitle{Preliminaries II}

  \begin{table}
    \begin{tabular}{c|cccccc}
      $D$ & \color{red}$X_1$ & \color{red}$X_2$ & \color{red}$X_3$ & \color{red}$X_4$ &
      \color{red}$X_5$ & \color{red}$X_6$\\
      \hline
      \color{blue}$I_1$ & 1 & 0 & 1 & 1 & 0 & 1\\
      \color{blue}$I_2$ & 0 & 1 & 0 & 0 & 1 & 0\\
    \end{tabular}
  \end{table}

  \begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
      \centering
      \begin{tikzpicture}
        \draw[fill=black!40!white] (1,0) rectangle (2,2);
        \draw[draw=black] (0,0) grid (3,2);
        \foreach \x in {0,...,2}
          \foreach \y in {0,...,1}
            {\pgfmathtruncatemacro{\label}{\x - 3 * \y + 4}
            \node at (\x+0.5,\y+0.5) {\color{red}$X_{\label}$};}
      \end{tikzpicture}
      \captionsetup{justification=centering}
      \caption*{\color{blue}$I_1$}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\linewidth}
      \centering
      \begin{tikzpicture}
        \draw[fill=black!40!white] (0,0) rectangle (1,2);
        \draw[fill=black!40!white] (2,0) rectangle (3,2);
        \draw[draw=black] (0,0) grid (3,2);
        \foreach \x in {0,...,2}
          \foreach \y in {0,...,1}
            {\pgfmathtruncatemacro{\label}{\x - 3 * \y + 4}
            \node at (\x+0.5,\y+0.5) {\color{red}$X_{\label}$};}
      \end{tikzpicture}
      \captionsetup{justification=centering}
      \caption*{\color{blue}$I_2$}
    \end{subfigure}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{LearnSPN I}
  {\large\textbf{LearnSPN} (\cite{gens-domingos})}

  The Gens-Domingos LearnSPN schema exploits two SPN properties:
  {\color{blue}\textbf{completeness}} and {\color{red}\textbf{decomposability}}.

  \begin{description}
    \item[\color{blue}Completeness:]~\\
      Interpret sum children as clusters (similar instances).
    \item[\color{red}Decomposability:]~\\
      Interpret product children as independent sets (disjoint scopes).
  \end{description}

  \textbf{Idea:}
  \begin{enumerate}
    \item {\color{blue}Partition by row through clustering.}
    \item {\color{red}Partition by column through variable independence tests.}
    \item Base case: univariate distribution.
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{LearnSPN II}

  \begin{algorithm}[H]
    \caption{\code{LearnSPN}: Gens-Domingos structure learning schema}
    \begin{algorithmic}[1]
      \Require Set of instances $I$ and scope $X$
      \Ensure SPN structure learned from $I$ and $X$
      \If{$|X|=1$}
        \State \textbf{return} univariate distribution over $I[X]$
      \EndIf%
      \State Partition $X$ into $P_1,P_2,\ldots,P_m$ st $\forall i, j$, $i\neq j$, $P_i\indep P_j$
      \If{$m>1$}
        \State \textbf{return} $\prod_i$\mcode{LearnSPN$(D,P_i)$}
      \EndIf%
      \State Cluster $I$ such that $Q_1,Q_2,\ldots,Q_n$ are $I$'s clusters
      \State \textbf{return} $\sum_i\frac{|Q_i|}{|I|}$\mcode{LearnSPN$(Q_i,X)$}
    \end{algorithmic}
  \end{algorithm}

\end{frame}

\begin{frame}
  \frametitle{LearnSPN III}

  \begin{figure}
    \centering\includegraphics[height=0.85\textheight]{imgs/learnspn.png}
  \end{figure}\vspace{-0.5cm}
  \blfootnote{Source:~\cite{gens-domingos}}
\end{frame}

\begin{frame}
  \frametitle{LearnSPN IV}

  LearnSPN is very flexible and modular.

  \begin{description}
    \item[Clustering:]~\\
      $k$-means, $k$-mode, DBSCAN, $\ldots$
    \item[Variable independence:]~\\
      G-test, $\chi^2$ Pearson test, Mutual Information, $\ldots$
    \item[Univariate distribution:]~\\
      Multivariate, gaussian, mixture of gaussians, $\ldots$
  \end{description}
\end{frame}

\begin{frame}
  \frametitle{LearnSPN V}

  \begin{description}
    \item[Pros:]~\\
      \begin{itemize}
        \item Flexible implementation;
        \item Very deep architecture even with small training size;
        \item Guarantees completeness and decomposability.
      \end{itemize}
    \item[Cons:]~\\
      \begin{itemize}
        \item Generates only trees;
        \item Not as expressive as dense and deep networks;
        \item Tree-like structure and deepness impact inference speed.
      \end{itemize}
  \end{description}
\end{frame}

\begin{frame}
  \frametitle{Poon dense architecture I}

  {\large\textbf{Poon-Domingos architecture} (\cite{poon-domingos})}

  Decompose data into local spatial \emph{neighborhoods}. Call neighborhoods \textbf{Regions},
  and represent them as sets of sums. \textbf{Decomposition} is done with products.

  \textbf{Idea:}
  \begin{enumerate}
    \item Define regions as set of sums.
    \item Define decompositions of regions as products.
    \item Create gaussian mixtures for leaves.
    \item Generate dense architecture through decomposition of regions.
    \item Learn weights with EM or Gradient Descent (GD).
    \item Prune zero weights and redundant sub-SPNs.
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Poon dense architecture II}

  \begin{example}~\\
    In the image domain, regions could be a rectangular set of pixels. Take our previous example.

    \begin{figure}
      \centering
      \begin{tikzpicture}
        \draw[fill=black!40!white] (1,0) rectangle (2,2);
        \draw[draw=black] (0,0) grid (3,2);
        \foreach \x in {0,...,2}
          \foreach \y in {0,...,1}
            {\pgfmathtruncatemacro{\label}{\x - 3 * \y + 4}
            \node at (\x+0.5,\y+0.5) {\color{red}$X_{\label}$};}
      \end{tikzpicture}
    \end{figure}

    Any rectangular subset of the image is a region. The smallest (e.g. one pixel) regions are
    called \emph{fine} regions. All others are \emph{coarse} regions.\qed
  \end{example}

  \textbf{Special case:} the whole data is a region, represented by a single sum node.
\end{frame}

\begin{frame}
  \frametitle{Poon dense architecture III}

  For each region, find all possible rectangular 2-decompositions (horizontal and vertical) and
  connect them with $m$ products.

  \begin{figure}
    \centering
    \begin{tikzpicture}
      \draw[fill=black!40!white] (1,0) rectangle (2,2);
      \draw[draw=black] (0,0) grid (3,2);
      \node[circle,draw,inner sep=1pt] (p1) at (-2.5, -1) {$\times$};
      \node at (-1.0, -1) {$\ldots$};
      \node[circle,draw,inner sep=1pt] (p2) at (0.5, -1) {$\times$};
      \node[circle,draw,inner sep=1pt] (p3) at (2.5, -1) {$\times$};
      \node at (4.0, -1) {$\ldots$};
      \node[circle,draw,inner sep=1pt] (p4) at (5.5, -1) {$\times$};
      \node (r1) at (1.5, 0)   {};
      \node (r2) at (-2.5, -2) {};
      \node (r3) at (0.0, -2)  {};
      \node (r4) at (3.0, -2)  {};
      \node (r5) at (5.5, -2)  {};
      \draw[fill=black!40!white] (-1, -4) rectangle (0, -2);
      \draw[draw=black] (-3,-4) grid (-2, -2);
      \draw[draw=black] (-1, -4) grid (1, -2);
      \node at (1.5, -3) {$\ldots$};
      \draw[fill=black!40!white] (3, -4) rectangle (4, -2);
      \draw[draw=black] (2, -4) grid (4, -2);
      \draw[draw=black] (5, -4) grid (6, -2);
      \begin{scope}[every path/.style={->}]
        \draw (r1) -- (p1);
        \draw (r1) -- (p2);
        \draw (r1) -- (p3);
        \draw (r1) -- (p4);
        \draw (p1) -- (r2);
        \draw (p1) -- (r3);
        \draw (p2) -- (r2);
        \draw (p2) -- (r3);
        \draw (p3) -- (r4);
        \draw (p3) -- (r5);
        \draw (p4) -- (r4);
        \draw (p4) -- (r5);
      \end{scope}
    \end{tikzpicture}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Poon dense architecture IV}

  \begin{description}
    \item[Pros:]~\\
      \begin{itemize}
        \item Very dense architecture;
        \item Depth of $2(d-1)$ for $d\times d$ images, though not as deep as LearnSPN;
        \item Fast to generate dense structure.
      \end{itemize}
    \item[Cons:]~\\
      \begin{itemize}
        \item Restricted to only local spatial neighborhoods;
        \item Very short and very long paths from root to a same variable;
        \item Biased for Viterbi-style inference (e.g. Max-Product for MAP), prefering shorter
          paths;
      \end{itemize}
  \end{description}
\end{frame}

\begin{frame}
  \frametitle{Region graphs}

  Both \textcolor{YellowOrange}{Dennis-Ventura (DV)} and RAT-SPNs use \textbf{Region Graphs} to
  construct an SPN.

  Derived from \textcolor{OliveGreen}{Poon-Domingos' (PD)} concept of Region and Decomposition.
  Introduced by DV.

  \begin{itemize}
    \item \textbf{Regions:}\\
      \textcolor{OliveGreen}{rectangular spatial neighborhoods} vs \textcolor{YellowOrange}{any
      neighborhood of variables};
    \item \textbf{Subregions:}\\
      \textcolor{OliveGreen}{fixed rectangular decomposition} vs \textcolor{YellowOrange}{any
      partitioning of the scope};
    \item \textbf{Structure:}\\
      \textcolor{OliveGreen}{depth is domain dependent} vs \textcolor{YellowOrange}{depth depends
      only on data/instances}.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Region graph definition}

  \begin{definition}[Region graph]~\\
    A region graph $G$ is a rooted DAG with two possible node types: region and partition nodes.
    The root of $G$ is a region node. Children of region nodes are partition nodes and vice-versa.
    Scope of $G$ works similarly to complete decomposable SPNs, with region nodes resembling sums
    and partitions products.
  \end{definition}

  \textbf{Main ideas:}
  \begin{itemize}
    \item Regions act like set of sums;
    \item Partitions act like set of products;
    \item Partition nodes partition the parent's scope;
    \item Regions are the result of this decomposition;
    \item Region graph is always a tree because of partitioning.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Constructing a region graph}

  \textbf{General idea for constructing a region graph:}
  \begin{enumerate}
    \item Create a root region from whole data.
    \item Partition data $k$ times.
    \item Construct partition node for each $k$ split.
    \item For each of these $k$, create two subregions representing split scope as children of
      corresponding partition node.
    \item Recursively repeat until early stop or reach scope of size 1.
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Converting a region graph to an SPN}

  \begin{align*}
    \text{Region node }R\quad &\longrightarrow\quad
    \begin{cases}
      m\text{ sum nodes,} & \quad \text{if coarse region;}\\
      d\text{ distributions,} & \quad \text{if fine region};
    \end{cases}~\\
    \text{Partition node }P\quad &\longrightarrow\quad m^2\text{ product nodes}
  \end{align*}

  Connect every parent region sum node to every child partition product node. ($R\to P$)

  Connect each partition product node with a distinct pair of children region nodes. ($P\to R$)
\end{frame}

\begin{frame}
  \frametitle{Semantics of a region graph}
\end{frame}

\begin{frame}[standout]
  \textbf{Thank you.\\~\\Questions?}
\end{frame}

\begin{frame}[t,allowframebreaks]
  \frametitle{References}
  \printbibliography[heading=none]
\end{frame}

\end{document}
